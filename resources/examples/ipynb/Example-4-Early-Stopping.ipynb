{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "colab": {
   "name": "earlystopping.ipynb",
   "provenance": [],
   "collapsed_sections": []
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "6NWA5uxOmBVz"
   },
   "source": [
    "#@title\n",
    "# Copyright 2020 Google LLC.\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r9WfLoXBP6Hc",
    "ExecuteTime": {
     "end_time": "2025-04-10T10:13:40.409564Z",
     "start_time": "2025-04-10T10:13:40.399939Z"
    }
   },
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-04T15:35:06.758715Z",
     "iopub.status.busy": "2020-12-04T15:35:06.758464Z",
     "iopub.status.idle": "2020-12-04T15:35:37.278247Z",
     "shell.execute_reply": "2020-12-04T15:35:37.277568Z",
     "shell.execute_reply.started": "2020-12-04T15:35:06.758651Z"
    },
    "id": "OLUMD0tPP6Hd",
    "ExecuteTime": {
     "end_time": "2025-04-10T10:13:42.489881Z",
     "start_time": "2025-04-10T10:13:42.298384Z"
    }
   },
   "source": [
    "import collections\n",
    "import functools\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import psutil\n",
    "from absl import logging"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T10:14:04.954899Z",
     "start_time": "2025-04-10T10:13:44.457143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# For example, if trax is inside a 'src' directory\n",
    "project_root = os.environ.get('TRAX_PROJECT_ROOT', '')\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "# Option to verify the import path\n",
    "print(f\"Python will look for packages in: {sys.path[0]}\")\n",
    "\n",
    "# Import trax\n",
    "import trax\n",
    "from trax.data.encoder import encoder\n",
    "from trax import fastmath\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as jnp\n",
    "from trax.learning.supervised import training\n",
    "\n",
    "# Verify the source of the imported package\n",
    "print(f\"Imported trax from: {trax.__file__}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python will look for packages in: /raid/mmironczuk/projects/trax-upgrade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 12:13:44.765221: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-10 12:13:44.786231: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-10 12:13:44.792704: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-10 12:13:44.809811: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-10 12:13:46.246271: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported trax from: /raid/mmironczuk/projects/trax-upgrade/trax/__init__.py\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nG4CK5NsP6He",
    "ExecuteTime": {
     "end_time": "2025-04-10T10:14:05.123712Z",
     "start_time": "2025-04-10T10:14:05.105552Z"
    }
   },
   "source": [
    "class MyLoop(training.Loop):\n",
    "    def __init__(\n",
    "            self,\n",
    "            *args, **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            *args, **kwargs\n",
    "        )\n",
    "        self._stop_training = False\n",
    "\n",
    "    def run(self, n_steps=1):\n",
    "        \"\"\"Just add a logic to break the loop to ``training.Loop.run`` when\n",
    "            the early stopping condition is satisfied.\n",
    "        \"\"\"\n",
    "\n",
    "        with self._open_summary_writers() as (\n",
    "                train_summary_writers,\n",
    "                eval_summary_writers,\n",
    "        ):\n",
    "            process = psutil.Process(os.getpid())\n",
    "            loss_acc, step_acc = 0.0, 0\n",
    "            start_time = time.time()\n",
    "            optimizer_metrics_acc = collections.defaultdict(float)\n",
    "            for i in range(n_steps):\n",
    "                prev_task_index = self._which_task(self._step)\n",
    "                self._step += 1\n",
    "                task_index = self._which_task(self._step)\n",
    "                task_changed = task_index != prev_task_index\n",
    "\n",
    "                if task_changed:\n",
    "                    loss_acc, step_acc = 0.0, 0\n",
    "\n",
    "                loss, optimizer_metrics = self._run_one_step(task_index, task_changed)\n",
    "\n",
    "                optimizer_metrics, loss = fastmath.nested_map(\n",
    "                    functools.partial(tl.mean, self._n_devices),\n",
    "                    (optimizer_metrics, loss),\n",
    "                )\n",
    "\n",
    "                loss_acc += loss\n",
    "                # Log loss every 50 steps, every step in memory-efficient trainers.\n",
    "                if self._step % 50 == 0 or self._use_memory_efficient_trainer:\n",
    "                    self._log_step(\"Loss: %.4f\" % loss, stdout=False)\n",
    "                step_acc += 1\n",
    "                for metric_name, value in optimizer_metrics.items():\n",
    "                    optimizer_metrics_acc[metric_name] += value\n",
    "\n",
    "                if self._checkpoint_at(self.step):\n",
    "                    self.save_checkpoint(\"model\")\n",
    "                if self._permanent_checkpoint_at(self.step):\n",
    "                    self.save_checkpoint(f\"model_{self.step}\")\n",
    "                if self._eval_at(self.step):\n",
    "                    logging.info(\n",
    "                        \"cpu memory use (MB): %.2f\",\n",
    "                        process.memory_info().rss / float(1024 * 1024),\n",
    "                    )\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    self._log_training_progress(\n",
    "                        task=self._tasks[task_index],\n",
    "                        total_loss=loss_acc,\n",
    "                        n_steps=step_acc,\n",
    "                        elapsed_time=elapsed_time,\n",
    "                        optimizer_metrics=optimizer_metrics_acc,\n",
    "                        summary_writer=train_summary_writers[task_index],\n",
    "                    )\n",
    "                    self.run_evals(eval_summary_writers)\n",
    "                    loss_acc, step_acc = 0.0, 0\n",
    "                    start_time = time.time()\n",
    "                    optimizer_metrics_acc = collections.defaultdict(float)\n",
    "\n",
    "                if self._checkpoint_at(self.step):\n",
    "                    if self._checkpoint_low_metric is not None and self._at_lowest():\n",
    "                        self.save_checkpoint(f\"lowest_{self._checkpoint_low_metric}\")\n",
    "                    if self._checkpoint_high_metric is not None and self._at_highest():\n",
    "                        self.save_checkpoint(f\"highest_{self._checkpoint_high_metric}\")\n",
    "\n",
    "                for callback in self._callbacks:\n",
    "                    if callback.call_at(self.step):\n",
    "                        if callback.__class__.__name__ == 'EarlyStopping':\n",
    "                            #added to check for earlystopping callback after\n",
    "                            # history was updated.\n",
    "                            #callback.on_step_end execute before history was\n",
    "                            #updated.\n",
    "                            best_step = callback.on_step_begin_with_history(self.step)\n",
    "\n",
    "                            if not self._stop_training and self.step == n_steps:\n",
    "                                self._log_step(\"Did not meet early stopping condition.\")\n",
    "\n",
    "                if self._stop_training:\n",
    "                    # added to stop the training.\n",
    "                    self._log_step(f\"Early stopping... \"\n",
    "                                   f\" the best step at {best_step}\")\n",
    "                    break\n",
    "\n",
    "        self._eval_model.weights = self._model.weights"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rfncVhM7P6Hg",
    "ExecuteTime": {
     "end_time": "2025-04-10T10:14:10.349698Z",
     "start_time": "2025-04-10T10:14:10.309218Z"
    }
   },
   "source": [
    "def callback_earlystopper(\n",
    "        monitor=None,\n",
    "        min_delta=0,\n",
    "        patience=0,\n",
    "        mode=\"auto\",\n",
    "        restore_best_checkpoint=True\n",
    "):\n",
    "    \"\"\"Wrap the EarlyStopping class into a callable.\n",
    "\n",
    "    Returns an early stopping.\n",
    "\n",
    "    Args:\n",
    "    monitor: Quantity to be monitored.\n",
    "\n",
    "    min_delta: Minimum change in the monitored quantity\n",
    "        to qualify as an improvement, i.e. an absolute\n",
    "        change of less than min_delta, will count as no\n",
    "        improvement.\n",
    "\n",
    "    patience: ``patience`` times ``n_steps_per_checkpoint`` will be\n",
    "        the total number of steps without improvement\n",
    "        after which training will be stopped.\n",
    "\n",
    "    mode: One of ``{\"auto\", \"min\", \"max\"}``. In ``min``(``max``) mode,\n",
    "        training will stop when the quantity monitored has stopped\n",
    "        decreasing(increasing) during the number of steps assigned\n",
    "        in ``patience``; in ``\"auto\"``\n",
    "        mode, the direction is automatically inferred\n",
    "        from the name of the monitored quantity.\n",
    "\n",
    "    restore_best_checkpoint: Whether to restore model from\n",
    "        the checkpoint with the best value of the monitored quantity.\n",
    "        If False, the model weights obtained at the last step of\n",
    "        training are used. If True and there is an early stopping,\n",
    "        the best checkpoint will be restored.\n",
    "    \"\"\"\n",
    "\n",
    "    if mode not in [\"auto\", \"max\", \"min\"]:\n",
    "        self._loop._log_step(\n",
    "            f\"Early stopping mode='{mode}' is unknown, \" \"fallback to 'auto' mode\"\n",
    "        )\n",
    "        mode = \"auto\"\n",
    "\n",
    "    class EarlyStopping:\n",
    "        \"\"\"Create a call back taht activates early stopping.\n",
    "\n",
    "        Activate early stopping.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, loop):\n",
    "            \"\"\"Configures an early stopping.\n",
    "            This is inspired by keras.callbacks.EarlyStopping.\n",
    "\n",
    "            Args:\n",
    "                loop:   training ``Loop`` from the current training.\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            self._loop = loop\n",
    "            self.monitor = monitor\n",
    "            self.min_delta = jnp.abs(min_delta)\n",
    "            self.patience = jnp.maximum(patience, 1)\n",
    "\n",
    "            self.restore_best_checkpoint = restore_best_checkpoint\n",
    "\n",
    "            if mode == \"min\":\n",
    "                self.monitor_op = jnp.less\n",
    "            elif mode == \"max\":\n",
    "                self.monitor_op = jnp.greater\n",
    "            else:\n",
    "                if self.monitor.endswith(\"Accuracy\"):\n",
    "                    self.monitor_op = jnp.greater\n",
    "                else:\n",
    "                    self.monitor_op = jnp.less\n",
    "\n",
    "            if self.monitor_op == np.greater:\n",
    "                self.min_delta *= 1\n",
    "            else:\n",
    "                self.min_delta *= -1\n",
    "\n",
    "            self.wait = 0\n",
    "            self.stopped_step = 1\n",
    "            self.best = jnp.inf if self.monitor_op == jnp.less else -jnp.inf\n",
    "            self.best_step = 1\n",
    "            self.best_checkpoint_path = None\n",
    "\n",
    "        def _is_metric_exist(self):\n",
    "            metric_names = [\n",
    "                name\n",
    "                for eval_task in self._loop._eval_tasks\n",
    "                for name in eval_task.metric_names\n",
    "            ]\n",
    "            return self.monitor in metric_names\n",
    "\n",
    "        def call_at(self, step):\n",
    "            return self._loop._eval_at(step)\n",
    "\n",
    "        def on_step_begin(self, step):\n",
    "            if not self._is_metric_exist():\n",
    "                # Raise error if the monitor name is not in evaluation task.\n",
    "                self._loop._log_step(\n",
    "                    f\"Early Stopping metric '{self.monitor}' \" \"is not in eval_tasks.\"\n",
    "                )\n",
    "                self._loop._log_step(\n",
    "                    \"Select one of \" f\"them from here {self.metric_names}.\"\n",
    "                )\n",
    "\n",
    "                raise SystemExit(\"Monitoring metric not found.\")\n",
    "\n",
    "        def on_step_end(self, step):\n",
    "            pass\n",
    "\n",
    "        def on_step_begin_with_history(self, step):\n",
    "            if self.restore_best_checkpoint and self.best_checkpoint_path is None:\n",
    "                self._loop.save_checkpoint(\"best_checkpoint\")\n",
    "                self.best_checkpoint_path = os.path.join(\n",
    "                    self._loop._output_dir, \"best_checkpoint.pkl.gz\"\n",
    "                )\n",
    "\n",
    "            self.wait += 1\n",
    "            current_step, current = self._get_monitor_value()\n",
    "\n",
    "            if current is None:\n",
    "                return\n",
    "\n",
    "            if self._is_improvement(current, self.best):\n",
    "                self.best = current\n",
    "                self.best_step = current_step\n",
    "                self._loop.save_checkpoint(\"best_checkpoint\")\n",
    "\n",
    "                # reset wait\n",
    "                self.wait = 0\n",
    "\n",
    "            if self.wait >= self.patience and step > 1:\n",
    "                self.stopped_step = current_step\n",
    "                self._loop._stop_training = True\n",
    "\n",
    "                if (\n",
    "                        self.restore_best_checkpoint\n",
    "                        and self.best_checkpoint_path is not None\n",
    "                ):\n",
    "                    self._loop.load_checkpoint(self.best_checkpoint_path)\n",
    "                    self._loop._log_step(\n",
    "                        f\"Best checkpoint was restored from Step {self.best_step}.\"\n",
    "                    )\n",
    "\n",
    "                return self.best_step\n",
    "\n",
    "        def _is_improvement(self, monitor_value, reference_value):\n",
    "            return self.monitor_op(monitor_value - self.min_delta, reference_value)\n",
    "\n",
    "        def _get_monitor_value(self):\n",
    "            step, monitor_value = self._loop.history.get(\n",
    "                \"eval\", \"metrics/\" + self.monitor\n",
    "            )[-1]\n",
    "            return step, monitor_value\n",
    "\n",
    "    return EarlyStopping"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJHUx_nSP6Hh"
   },
   "source": [
    "## Linear Regression\n",
    "## Generate data for linear model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-04T15:35:37.279761Z",
     "iopub.status.busy": "2020-12-04T15:35:37.279529Z",
     "iopub.status.idle": "2020-12-04T15:35:37.283375Z",
     "shell.execute_reply": "2020-12-04T15:35:37.282592Z",
     "shell.execute_reply.started": "2020-12-04T15:35:37.279738Z"
    },
    "id": "dKYZQY-pP6Hi",
    "ExecuteTime": {
     "end_time": "2025-04-10T10:14:49.356346Z",
     "start_time": "2025-04-10T10:14:49.348014Z"
    }
   },
   "source": [
    "def get_data_linear():\n",
    "    while True:\n",
    "        x = np.random.randint(low=1, high=10) * 1.0\n",
    "        y = x * 2.0 - 1\n",
    "        yield (np.array([x]), np.array([y]))"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SCTZW1pBP6Hj",
    "ExecuteTime": {
     "end_time": "2025-04-10T10:14:52.041584Z",
     "start_time": "2025-04-10T10:14:52.033786Z"
    }
   },
   "source": [
    "data_linear = get_data_linear()\n",
    "print(next(data_linear))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([7.]), array([13.]))\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-04T15:35:37.292101Z",
     "iopub.status.busy": "2020-12-04T15:35:37.291815Z",
     "iopub.status.idle": "2020-12-04T15:35:37.296048Z",
     "shell.execute_reply": "2020-12-04T15:35:37.295266Z",
     "shell.execute_reply.started": "2020-12-04T15:35:37.292054Z"
    },
    "id": "4pcAhWJMP6Hk",
    "ExecuteTime": {
     "end_time": "2025-04-10T10:16:17.301505Z",
     "start_time": "2025-04-10T10:16:17.294182Z"
    }
   },
   "source": [
    "from trax.data.preprocessing import inputs as preprocessing\n",
    "\n",
    "data_pipeline = preprocessing.Serial(preprocessing.Batch(50), preprocessing.AddLossWeights(), )\n",
    "data_stream = data_pipeline(data_linear)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vK15-1oP6Hl"
   },
   "source": [
    "## Build a simple linear model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xzN0oZBCP6Hl",
    "ExecuteTime": {
     "end_time": "2025-04-10T10:16:20.374825Z",
     "start_time": "2025-04-10T10:16:20.368272Z"
    }
   },
   "source": [
    "model_linear = tl.Serial(tl.Dense(1))"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qi0bM41PP6Hl"
   },
   "source": [
    "## Train a linear model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-04T15:35:37.302605Z",
     "iopub.status.busy": "2020-12-04T15:35:37.302292Z",
     "iopub.status.idle": "2020-12-04T15:35:37.311629Z",
     "shell.execute_reply": "2020-12-04T15:35:37.311016Z",
     "shell.execute_reply.started": "2020-12-04T15:35:37.302575Z"
    },
    "id": "d0_9qZHVP6Hm",
    "ExecuteTime": {
     "end_time": "2025-04-10T10:17:13.066902Z",
     "start_time": "2025-04-10T10:17:12.137121Z"
    }
   },
   "source": [
    "from trax import optimizers as optimizers\n",
    "\n",
    "# Use the same data_stream for both training and evaluation\n",
    "train_task = training.TrainTask(\n",
    "    labeled_data=data_stream,\n",
    "    loss_layer=tl.L2Loss(),\n",
    "    optimizer=optimizers.SGD(0.01),\n",
    "    n_steps_per_checkpoint=10,\n",
    ")\n",
    "\n",
    "eval_task = training.EvalTask(\n",
    "    labeled_data=data_stream, metrics=[tl.L2Loss()], n_eval_batches=15,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5ngyoYSP6Hm"
   },
   "source": [
    "## Add early stopping function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SKetNF4LP6Hm",
    "ExecuteTime": {
     "end_time": "2025-04-10T10:17:17.351329Z",
     "start_time": "2025-04-10T10:17:17.347134Z"
    }
   },
   "source": "earlystopping = callback_earlystopper(monitor='L2Loss', min_delta=1e-4)",
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-04T15:35:37.313247Z",
     "iopub.status.busy": "2020-12-04T15:35:37.313032Z",
     "iopub.status.idle": "2020-12-04T15:35:37.442811Z",
     "shell.execute_reply": "2020-12-04T15:35:37.442187Z",
     "shell.execute_reply.started": "2020-12-04T15:35:37.313221Z"
    },
    "id": "D2XjQO80P6Hn",
    "ExecuteTime": {
     "end_time": "2025-04-10T10:17:22.632497Z",
     "start_time": "2025-04-10T10:17:22.438656Z"
    }
   },
   "source": [
    "# Delete the training folder\n",
    "!rm -r linear_model"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "rm: cannot remove 'linear_model': No such file or directory\r\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-04T15:35:37.444083Z",
     "iopub.status.busy": "2020-12-04T15:35:37.443918Z",
     "iopub.status.idle": "2020-12-04T15:35:39.043136Z",
     "shell.execute_reply": "2020-12-04T15:35:39.042484Z",
     "shell.execute_reply.started": "2020-12-04T15:35:37.444063Z"
    },
    "id": "mCrc_bXZP6Hn",
    "ExecuteTime": {
     "end_time": "2025-04-10T10:17:29.325162Z",
     "start_time": "2025-04-10T10:17:27.577785Z"
    }
   },
   "source": [
    "model_linear = tl.Serial(tl.Dense(1))\n",
    "training_loop = MyLoop(\n",
    "    model=model_linear, tasks=train_task, eval_tasks=[eval_task], output_dir=\"./linear_model\",\n",
    "    callbacks=[earlystopping]\n",
    ")\n",
    "# training_loop.save_checkpoint(f'step_{training_loop.step}')"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kFURD6T4P6Hn",
    "ExecuteTime": {
     "end_time": "2025-04-10T10:17:36.882884Z",
     "start_time": "2025-04-10T10:17:34.832741Z"
    }
   },
   "source": [
    "training_loop.run(1500)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step      1: Total number of trainable weights: 2\n",
      "Step      1: Ran 1 train steps in 0.26 secs\n",
      "Step      1: train L2Loss |  7.69807005\n",
      "Step      1: eval  L2Loss |  0.63437390\n",
      "\n",
      "Step     10: Ran 9 train steps in 0.26 secs\n",
      "Step     10: train L2Loss |  0.29812446\n",
      "Step     10: eval  L2Loss |  0.23468615\n",
      "\n",
      "Step     20: Ran 10 train steps in 0.12 secs\n",
      "Step     20: train L2Loss |  0.21997391\n",
      "Step     20: eval  L2Loss |  0.20311840\n",
      "\n",
      "Step     30: Ran 10 train steps in 0.08 secs\n",
      "Step     30: train L2Loss |  0.19210428\n",
      "Step     30: eval  L2Loss |  0.18424661\n",
      "\n",
      "Step     40: Ran 10 train steps in 0.08 secs\n",
      "Step     40: train L2Loss |  0.18213718\n",
      "Step     40: eval  L2Loss |  0.17369503\n",
      "\n",
      "Step     50: Ran 10 train steps in 0.08 secs\n",
      "Step     50: train L2Loss |  0.15699960\n",
      "Step     50: eval  L2Loss |  0.15382139\n",
      "\n",
      "Step     60: Ran 10 train steps in 0.09 secs\n",
      "Step     60: train L2Loss |  0.15296355\n",
      "Step     60: eval  L2Loss |  0.12982111\n",
      "\n",
      "Step     70: Ran 10 train steps in 0.09 secs\n",
      "Step     70: train L2Loss |  0.13677828\n",
      "Step     70: eval  L2Loss |  0.13910370\n",
      "Step     70: Best checkpoint was restored from Step 60.\n",
      "Step     70: Early stopping...  the best step at 60\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lg_ONworP6Hn"
   },
   "source": [
    "## Change patience\n",
    "patience = 10 means it will wait for 10 x 10 = 100 steps (patience * n_steps_per_checkpoint ) to before making a decision to stop."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IStFKG7GP6Hn",
    "ExecuteTime": {
     "end_time": "2025-04-10T10:17:55.291247Z",
     "start_time": "2025-04-10T10:17:55.285388Z"
    }
   },
   "source": "earlystopping = callback_earlystopper(monitor='L2Loss', patience=10, min_delta=1e-4)",
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-04T15:35:37.313247Z",
     "iopub.status.busy": "2020-12-04T15:35:37.313032Z",
     "iopub.status.idle": "2020-12-04T15:35:37.442811Z",
     "shell.execute_reply": "2020-12-04T15:35:37.442187Z",
     "shell.execute_reply.started": "2020-12-04T15:35:37.313221Z"
    },
    "id": "pihrcvTtP6Ho",
    "ExecuteTime": {
     "end_time": "2025-04-10T10:17:56.870430Z",
     "start_time": "2025-04-10T10:17:56.669531Z"
    }
   },
   "source": [
    "# Delete the training folder\n",
    "!rm -r linear_model"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-04T15:35:37.444083Z",
     "iopub.status.busy": "2020-12-04T15:35:37.443918Z",
     "iopub.status.idle": "2020-12-04T15:35:39.043136Z",
     "shell.execute_reply": "2020-12-04T15:35:39.042484Z",
     "shell.execute_reply.started": "2020-12-04T15:35:37.444063Z"
    },
    "id": "UvjDLZd3P6Ho",
    "ExecuteTime": {
     "end_time": "2025-04-10T10:17:59.153636Z",
     "start_time": "2025-04-10T10:17:59.132715Z"
    }
   },
   "source": [
    "model_linear = tl.Serial(tl.Dense(1))\n",
    "training_loop = MyLoop(\n",
    "    model=model_linear, tasks=train_task, eval_tasks=[eval_task], output_dir=\"./linear_model\",\n",
    "    callbacks=[earlystopping]\n",
    ")\n",
    "# training_loop.save_checkpoint(f'step_{training_loop.step}')"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false,
    "id": "bAsft27BP6Ho",
    "ExecuteTime": {
     "end_time": "2025-04-10T10:18:28.917884Z",
     "start_time": "2025-04-10T10:18:00.895237Z"
    }
   },
   "source": [
    "training_loop.run(1500)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step      1: Total number of trainable weights: 2\n",
      "Step      1: Ran 1 train steps in 0.22 secs\n",
      "Step      1: train L2Loss |  18.23833466\n",
      "Step      1: eval  L2Loss |  2.60297537\n",
      "\n",
      "Step     10: Ran 9 train steps in 0.08 secs\n",
      "Step     10: train L2Loss |  0.56292963\n",
      "Step     10: eval  L2Loss |  0.23197113\n",
      "\n",
      "Step     20: Ran 10 train steps in 0.08 secs\n",
      "Step     20: train L2Loss |  0.23490067\n",
      "Step     20: eval  L2Loss |  0.22663632\n",
      "\n",
      "Step     30: Ran 10 train steps in 0.08 secs\n",
      "Step     30: train L2Loss |  0.21568297\n",
      "Step     30: eval  L2Loss |  0.20940009\n",
      "\n",
      "Step     40: Ran 10 train steps in 0.08 secs\n",
      "Step     40: train L2Loss |  0.20161334\n",
      "Step     40: eval  L2Loss |  0.18221980\n",
      "\n",
      "Step     50: Ran 10 train steps in 0.08 secs\n",
      "Step     50: train L2Loss |  0.17556223\n",
      "Step     50: eval  L2Loss |  0.17093813\n",
      "\n",
      "Step     60: Ran 10 train steps in 0.09 secs\n",
      "Step     60: train L2Loss |  0.16914175\n",
      "Step     60: eval  L2Loss |  0.15923102\n",
      "\n",
      "Step     70: Ran 10 train steps in 0.09 secs\n",
      "Step     70: train L2Loss |  0.16000053\n",
      "Step     70: eval  L2Loss |  0.14509904\n",
      "\n",
      "Step     80: Ran 10 train steps in 0.09 secs\n",
      "Step     80: train L2Loss |  0.13832577\n",
      "Step     80: eval  L2Loss |  0.13871129\n",
      "\n",
      "Step     90: Ran 10 train steps in 0.10 secs\n",
      "Step     90: train L2Loss |  0.13039386\n",
      "Step     90: eval  L2Loss |  0.12619935\n",
      "\n",
      "Step    100: Ran 10 train steps in 0.10 secs\n",
      "Step    100: train L2Loss |  0.11863101\n",
      "Step    100: eval  L2Loss |  0.12001261\n",
      "\n",
      "Step    110: Ran 10 train steps in 0.10 secs\n",
      "Step    110: train L2Loss |  0.11097759\n",
      "Step    110: eval  L2Loss |  0.10367471\n",
      "\n",
      "Step    120: Ran 10 train steps in 0.11 secs\n",
      "Step    120: train L2Loss |  0.09700185\n",
      "Step    120: eval  L2Loss |  0.10637346\n",
      "\n",
      "Step    130: Ran 10 train steps in 0.09 secs\n",
      "Step    130: train L2Loss |  0.09758825\n",
      "Step    130: eval  L2Loss |  0.08650282\n",
      "\n",
      "Step    140: Ran 10 train steps in 0.11 secs\n",
      "Step    140: train L2Loss |  0.08228681\n",
      "Step    140: eval  L2Loss |  0.08081486\n",
      "\n",
      "Step    150: Ran 10 train steps in 0.12 secs\n",
      "Step    150: train L2Loss |  0.08175614\n",
      "Step    150: eval  L2Loss |  0.07298367\n",
      "\n",
      "Step    160: Ran 10 train steps in 0.12 secs\n",
      "Step    160: train L2Loss |  0.07239142\n",
      "Step    160: eval  L2Loss |  0.07323325\n",
      "\n",
      "Step    170: Ran 10 train steps in 0.09 secs\n",
      "Step    170: train L2Loss |  0.06735802\n",
      "Step    170: eval  L2Loss |  0.06831225\n",
      "\n",
      "Step    180: Ran 10 train steps in 0.13 secs\n",
      "Step    180: train L2Loss |  0.06375156\n",
      "Step    180: eval  L2Loss |  0.06013875\n",
      "\n",
      "Step    190: Ran 10 train steps in 0.13 secs\n",
      "Step    190: train L2Loss |  0.06049114\n",
      "Step    190: eval  L2Loss |  0.05813525\n",
      "\n",
      "Step    200: Ran 10 train steps in 0.14 secs\n",
      "Step    200: train L2Loss |  0.05127042\n",
      "Step    200: eval  L2Loss |  0.04962815\n",
      "\n",
      "Step    210: Ran 10 train steps in 0.14 secs\n",
      "Step    210: train L2Loss |  0.04489381\n",
      "Step    210: eval  L2Loss |  0.04718936\n",
      "\n",
      "Step    220: Ran 10 train steps in 0.14 secs\n",
      "Step    220: train L2Loss |  0.04669733\n",
      "Step    220: eval  L2Loss |  0.04396832\n",
      "\n",
      "Step    230: Ran 10 train steps in 0.14 secs\n",
      "Step    230: train L2Loss |  0.04349758\n",
      "Step    230: eval  L2Loss |  0.03974083\n",
      "\n",
      "Step    240: Ran 10 train steps in 0.15 secs\n",
      "Step    240: train L2Loss |  0.03762129\n",
      "Step    240: eval  L2Loss |  0.03721851\n",
      "\n",
      "Step    250: Ran 10 train steps in 0.15 secs\n",
      "Step    250: train L2Loss |  0.03392620\n",
      "Step    250: eval  L2Loss |  0.03633574\n",
      "\n",
      "Step    260: Ran 10 train steps in 0.15 secs\n",
      "Step    260: train L2Loss |  0.03315348\n",
      "Step    260: eval  L2Loss |  0.03052452\n",
      "\n",
      "Step    270: Ran 10 train steps in 0.16 secs\n",
      "Step    270: train L2Loss |  0.02891695\n",
      "Step    270: eval  L2Loss |  0.02858864\n",
      "\n",
      "Step    280: Ran 10 train steps in 0.16 secs\n",
      "Step    280: train L2Loss |  0.02613866\n",
      "Step    280: eval  L2Loss |  0.02804898\n",
      "\n",
      "Step    290: Ran 10 train steps in 0.16 secs\n",
      "Step    290: train L2Loss |  0.02704040\n",
      "Step    290: eval  L2Loss |  0.02344525\n",
      "\n",
      "Step    300: Ran 10 train steps in 0.18 secs\n",
      "Step    300: train L2Loss |  0.02337220\n",
      "Step    300: eval  L2Loss |  0.02225683\n",
      "\n",
      "Step    310: Ran 10 train steps in 0.18 secs\n",
      "Step    310: train L2Loss |  0.02214986\n",
      "Step    310: eval  L2Loss |  0.01981945\n",
      "\n",
      "Step    320: Ran 10 train steps in 0.17 secs\n",
      "Step    320: train L2Loss |  0.02006903\n",
      "Step    320: eval  L2Loss |  0.01828980\n",
      "\n",
      "Step    330: Ran 10 train steps in 0.17 secs\n",
      "Step    330: train L2Loss |  0.01920871\n",
      "Step    330: eval  L2Loss |  0.01826261\n",
      "\n",
      "Step    340: Ran 10 train steps in 0.12 secs\n",
      "Step    340: train L2Loss |  0.01688296\n",
      "Step    340: eval  L2Loss |  0.01719620\n",
      "\n",
      "Step    350: Ran 10 train steps in 0.18 secs\n",
      "Step    350: train L2Loss |  0.01550694\n",
      "Step    350: eval  L2Loss |  0.01495000\n",
      "\n",
      "Step    360: Ran 10 train steps in 0.19 secs\n",
      "Step    360: train L2Loss |  0.01444746\n",
      "Step    360: eval  L2Loss |  0.01261852\n",
      "\n",
      "Step    370: Ran 10 train steps in 0.19 secs\n",
      "Step    370: train L2Loss |  0.01359655\n",
      "Step    370: eval  L2Loss |  0.01209912\n",
      "\n",
      "Step    380: Ran 10 train steps in 0.20 secs\n",
      "Step    380: train L2Loss |  0.01276465\n",
      "Step    380: eval  L2Loss |  0.01179450\n",
      "\n",
      "Step    390: Ran 10 train steps in 0.20 secs\n",
      "Step    390: train L2Loss |  0.01140201\n",
      "Step    390: eval  L2Loss |  0.00992139\n",
      "\n",
      "Step    400: Ran 10 train steps in 0.20 secs\n",
      "Step    400: train L2Loss |  0.00991733\n",
      "Step    400: eval  L2Loss |  0.01000021\n",
      "\n",
      "Step    410: Ran 10 train steps in 0.13 secs\n",
      "Step    410: train L2Loss |  0.00977403\n",
      "Step    410: eval  L2Loss |  0.00897356\n",
      "\n",
      "Step    420: Ran 10 train steps in 0.21 secs\n",
      "Step    420: train L2Loss |  0.00832709\n",
      "Step    420: eval  L2Loss |  0.00862052\n",
      "\n",
      "Step    430: Ran 10 train steps in 0.21 secs\n",
      "Step    430: train L2Loss |  0.00815371\n",
      "Step    430: eval  L2Loss |  0.00715209\n",
      "\n",
      "Step    440: Ran 10 train steps in 0.21 secs\n",
      "Step    440: train L2Loss |  0.00753400\n",
      "Step    440: eval  L2Loss |  0.00689937\n",
      "\n",
      "Step    450: Ran 10 train steps in 0.22 secs\n",
      "Step    450: train L2Loss |  0.00685729\n",
      "Step    450: eval  L2Loss |  0.00650191\n",
      "\n",
      "Step    460: Ran 10 train steps in 0.22 secs\n",
      "Step    460: train L2Loss |  0.00632967\n",
      "Step    460: eval  L2Loss |  0.00594764\n",
      "\n",
      "Step    470: Ran 10 train steps in 0.22 secs\n",
      "Step    470: train L2Loss |  0.00555230\n",
      "Step    470: eval  L2Loss |  0.00482079\n",
      "\n",
      "Step    480: Ran 10 train steps in 0.23 secs\n",
      "Step    480: train L2Loss |  0.00525765\n",
      "Step    480: eval  L2Loss |  0.00482282\n",
      "\n",
      "Step    490: Ran 10 train steps in 0.15 secs\n",
      "Step    490: train L2Loss |  0.00498673\n",
      "Step    490: eval  L2Loss |  0.00449872\n",
      "\n",
      "Step    500: Ran 10 train steps in 0.23 secs\n",
      "Step    500: train L2Loss |  0.00408058\n",
      "Step    500: eval  L2Loss |  0.00417434\n",
      "\n",
      "Step    510: Ran 10 train steps in 0.23 secs\n",
      "Step    510: train L2Loss |  0.00401137\n",
      "Step    510: eval  L2Loss |  0.00378141\n",
      "\n",
      "Step    520: Ran 10 train steps in 0.24 secs\n",
      "Step    520: train L2Loss |  0.00398008\n",
      "Step    520: eval  L2Loss |  0.00370479\n",
      "\n",
      "Step    530: Ran 10 train steps in 0.15 secs\n",
      "Step    530: train L2Loss |  0.00362712\n",
      "Step    530: eval  L2Loss |  0.00332374\n",
      "\n",
      "Step    540: Ran 10 train steps in 0.24 secs\n",
      "Step    540: train L2Loss |  0.00342630\n",
      "Step    540: eval  L2Loss |  0.00319429\n",
      "\n",
      "Step    550: Ran 10 train steps in 0.25 secs\n",
      "Step    550: train L2Loss |  0.00268633\n",
      "Step    550: eval  L2Loss |  0.00283975\n",
      "\n",
      "Step    560: Ran 10 train steps in 0.25 secs\n",
      "Step    560: train L2Loss |  0.00237337\n",
      "Step    560: eval  L2Loss |  0.00267291\n",
      "\n",
      "Step    570: Ran 10 train steps in 0.26 secs\n",
      "Step    570: train L2Loss |  0.00240752\n",
      "Step    570: eval  L2Loss |  0.00233746\n",
      "\n",
      "Step    580: Ran 10 train steps in 0.28 secs\n",
      "Step    580: train L2Loss |  0.00238994\n",
      "Step    580: eval  L2Loss |  0.00240203\n",
      "\n",
      "Step    590: Ran 10 train steps in 0.16 secs\n",
      "Step    590: train L2Loss |  0.00212477\n",
      "Step    590: eval  L2Loss |  0.00219436\n",
      "\n",
      "Step    600: Ran 10 train steps in 0.27 secs\n",
      "Step    600: train L2Loss |  0.00189184\n",
      "Step    600: eval  L2Loss |  0.00172079\n",
      "\n",
      "Step    610: Ran 10 train steps in 0.27 secs\n",
      "Step    610: train L2Loss |  0.00187565\n",
      "Step    610: eval  L2Loss |  0.00174039\n",
      "\n",
      "Step    620: Ran 10 train steps in 0.17 secs\n",
      "Step    620: train L2Loss |  0.00166475\n",
      "Step    620: eval  L2Loss |  0.00155617\n",
      "\n",
      "Step    630: Ran 10 train steps in 0.27 secs\n",
      "Step    630: train L2Loss |  0.00155597\n",
      "Step    630: eval  L2Loss |  0.00145297\n",
      "\n",
      "Step    640: Ran 10 train steps in 0.29 secs\n",
      "Step    640: train L2Loss |  0.00145353\n",
      "Step    640: eval  L2Loss |  0.00134310\n",
      "\n",
      "Step    650: Ran 10 train steps in 0.28 secs\n",
      "Step    650: train L2Loss |  0.00118677\n",
      "Step    650: eval  L2Loss |  0.00131491\n",
      "\n",
      "Step    660: Ran 10 train steps in 0.17 secs\n",
      "Step    660: train L2Loss |  0.00123379\n",
      "Step    660: eval  L2Loss |  0.00115531\n",
      "\n",
      "Step    670: Ran 10 train steps in 0.28 secs\n",
      "Step    670: train L2Loss |  0.00109319\n",
      "Step    670: eval  L2Loss |  0.00102347\n",
      "\n",
      "Step    680: Ran 10 train steps in 0.28 secs\n",
      "Step    680: train L2Loss |  0.00100554\n",
      "Step    680: eval  L2Loss |  0.00103667\n",
      "\n",
      "Step    690: Ran 10 train steps in 0.21 secs\n",
      "Step    690: train L2Loss |  0.00098995\n",
      "Step    690: eval  L2Loss |  0.00091016\n",
      "\n",
      "Step    700: Ran 10 train steps in 0.28 secs\n",
      "Step    700: train L2Loss |  0.00090362\n",
      "Step    700: eval  L2Loss |  0.00083551\n",
      "\n",
      "Step    710: Ran 10 train steps in 0.17 secs\n",
      "Step    710: train L2Loss |  0.00085842\n",
      "Step    710: eval  L2Loss |  0.00078628\n",
      "\n",
      "Step    720: Ran 10 train steps in 0.29 secs\n",
      "Step    720: train L2Loss |  0.00073422\n",
      "Step    720: eval  L2Loss |  0.00070911\n",
      "\n",
      "Step    730: Ran 10 train steps in 0.18 secs\n",
      "Step    730: train L2Loss |  0.00075125\n",
      "Step    730: eval  L2Loss |  0.00061879\n",
      "\n",
      "Step    740: Ran 10 train steps in 0.29 secs\n",
      "Step    740: train L2Loss |  0.00059485\n",
      "Step    740: eval  L2Loss |  0.00061083\n",
      "\n",
      "Step    750: Ran 10 train steps in 0.18 secs\n",
      "Step    750: train L2Loss |  0.00060110\n",
      "Step    750: eval  L2Loss |  0.00059493\n",
      "\n",
      "Step    760: Ran 10 train steps in 0.18 secs\n",
      "Step    760: train L2Loss |  0.00051659\n",
      "Step    760: eval  L2Loss |  0.00049139\n",
      "\n",
      "Step    770: Ran 10 train steps in 0.30 secs\n",
      "Step    770: train L2Loss |  0.00053577\n",
      "Step    770: eval  L2Loss |  0.00047668\n",
      "\n",
      "Step    780: Ran 10 train steps in 0.18 secs\n",
      "Step    780: train L2Loss |  0.00043499\n",
      "Step    780: eval  L2Loss |  0.00042753\n",
      "\n",
      "Step    790: Ran 10 train steps in 0.19 secs\n",
      "Step    790: train L2Loss |  0.00037675\n",
      "Step    790: eval  L2Loss |  0.00039033\n",
      "\n",
      "Step    800: Ran 10 train steps in 0.31 secs\n",
      "Step    800: train L2Loss |  0.00037836\n",
      "Step    800: eval  L2Loss |  0.00035951\n",
      "\n",
      "Step    810: Ran 10 train steps in 0.19 secs\n",
      "Step    810: train L2Loss |  0.00037718\n",
      "Step    810: eval  L2Loss |  0.00033747\n",
      "\n",
      "Step    820: Ran 10 train steps in 0.19 secs\n",
      "Step    820: train L2Loss |  0.00036243\n",
      "Step    820: eval  L2Loss |  0.00030153\n",
      "\n",
      "Step    830: Ran 10 train steps in 0.19 secs\n",
      "Step    830: train L2Loss |  0.00029442\n",
      "Step    830: eval  L2Loss |  0.00028130\n",
      "\n",
      "Step    840: Ran 10 train steps in 0.32 secs\n",
      "Step    840: train L2Loss |  0.00025560\n",
      "Step    840: eval  L2Loss |  0.00027985\n",
      "\n",
      "Step    850: Ran 10 train steps in 0.19 secs\n",
      "Step    850: train L2Loss |  0.00028045\n",
      "Step    850: eval  L2Loss |  0.00023485\n",
      "\n",
      "Step    860: Ran 10 train steps in 0.20 secs\n",
      "Step    860: train L2Loss |  0.00024655\n",
      "Step    860: eval  L2Loss |  0.00023763\n",
      "\n",
      "Step    870: Ran 10 train steps in 0.20 secs\n",
      "Step    870: train L2Loss |  0.00022995\n",
      "Step    870: eval  L2Loss |  0.00021085\n",
      "\n",
      "Step    880: Ran 10 train steps in 0.19 secs\n",
      "Step    880: train L2Loss |  0.00020189\n",
      "Step    880: eval  L2Loss |  0.00019520\n",
      "\n",
      "Step    890: Ran 10 train steps in 0.20 secs\n",
      "Step    890: train L2Loss |  0.00019576\n",
      "Step    890: eval  L2Loss |  0.00018492\n",
      "\n",
      "Step    900: Ran 10 train steps in 0.20 secs\n",
      "Step    900: train L2Loss |  0.00017356\n",
      "Step    900: eval  L2Loss |  0.00015802\n",
      "\n",
      "Step    910: Ran 10 train steps in 0.34 secs\n",
      "Step    910: train L2Loss |  0.00016176\n",
      "Step    910: eval  L2Loss |  0.00014086\n",
      "\n",
      "Step    920: Ran 10 train steps in 0.21 secs\n",
      "Step    920: train L2Loss |  0.00014325\n",
      "Step    920: eval  L2Loss |  0.00013011\n",
      "\n",
      "Step    930: Ran 10 train steps in 0.21 secs\n",
      "Step    930: train L2Loss |  0.00014360\n",
      "Step    930: eval  L2Loss |  0.00011918\n",
      "\n",
      "Step    940: Ran 10 train steps in 0.21 secs\n",
      "Step    940: train L2Loss |  0.00011783\n",
      "Step    940: eval  L2Loss |  0.00012001\n",
      "\n",
      "Step    950: Ran 10 train steps in 0.21 secs\n",
      "Step    950: train L2Loss |  0.00011428\n",
      "Step    950: eval  L2Loss |  0.00011346\n",
      "\n",
      "Step    960: Ran 10 train steps in 0.21 secs\n",
      "Step    960: train L2Loss |  0.00010210\n",
      "Step    960: eval  L2Loss |  0.00010017\n",
      "\n",
      "Step    970: Ran 10 train steps in 0.21 secs\n",
      "Step    970: train L2Loss |  0.00008965\n",
      "Step    970: eval  L2Loss |  0.00009744\n",
      "\n",
      "Step    980: Ran 10 train steps in 0.21 secs\n",
      "Step    980: train L2Loss |  0.00008396\n",
      "Step    980: eval  L2Loss |  0.00008466\n",
      "\n",
      "Step    990: Ran 10 train steps in 0.22 secs\n",
      "Step    990: train L2Loss |  0.00007567\n",
      "Step    990: eval  L2Loss |  0.00007560\n",
      "\n",
      "Step   1000: Ran 10 train steps in 0.22 secs\n",
      "Step   1000: train L2Loss |  0.00007629\n",
      "Step   1000: eval  L2Loss |  0.00007320\n",
      "Step   1000: Best checkpoint was restored from Step 900.\n",
      "Step   1000: Early stopping...  the best step at 900\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HyIjZWBP6Ho"
   },
   "source": "## Make a prediction"
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T04:36:10.040691Z",
     "iopub.status.busy": "2020-12-05T04:36:10.040407Z",
     "iopub.status.idle": "2020-12-05T04:36:10.114322Z",
     "shell.execute_reply": "2020-12-05T04:36:10.113606Z",
     "shell.execute_reply.started": "2020-12-05T04:36:10.040657Z"
    },
    "id": "d7bVzat7P6Ho",
    "ExecuteTime": {
     "end_time": "2025-04-10T10:18:34.682809Z",
     "start_time": "2025-04-10T10:18:34.538139Z"
    }
   },
   "source": [
    "test_data = np.array([[2.0], [3.0], [10.0], [44.0]])\n",
    "model_linear(test_data)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 3.0123417],\n",
       "       [ 5.009349 ],\n",
       "       [18.988398 ],\n",
       "       [86.88663  ]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  }
 ]
}
