{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hourglass_enwik8.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdO8Wj1U5tLN"
      },
      "source": [
        "#### Copyright 2021 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRaMWbn1BMtI"
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\")\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmkqZ5M69BJS"
      },
      "source": [
        "# Hourglass: enwik8 evaluation [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/trax/blob/master/trax/models/research/examples/hourglass_enwik8.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vlpvzGh-O2N"
      },
      "source": [
        "This notebook was designed to run on TPU.\n",
        "\n",
        "To use TPUs in Colab, click \"Runtime\" on the main menu bar and select Change runtime type. Set \"TPU\" as the hardware accelerator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXBTKGSsegOy"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaqpNaQ4ShRP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff3fac8d-88c8-42ae-cffe-a0aa5a51ab7f"
      },
      "source": [
        "TRAX_GITHUB_URL = 'git+https://github.com/google/trax.git'\n",
        "!pip install -q --upgrade jax==0.2.21\n",
        "!pip install -q --upgrade jaxlib==0.1.71+cuda111 -f https://storage.googleapis.com/jax-releases/jax_releases.html\n",
        "!pip install -q $TRAX_GITHUB_URL\n",
        "!pip install -q pickle5\n",
        "!pip install -q neptune-client\n",
        "!pip install -q gin"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVN8e_m9kg3K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b9f1d4f-a9db-4219-a196-3a8802d993ca"
      },
      "source": [
        "# Execute this for a proper TPU setup!\n",
        "# Make sure the Colab Runtime is set to Accelerator: TPU.\n",
        "import jax\n",
        "import requests\n",
        "import os\n",
        "if 'TPU_DRIVER_MODE' not in globals():\n",
        "    url = 'http://' + os.environ['COLAB_TPU_ADDR'].split(':')[0] + ':8475/requestversion/tpu_driver0.1-dev20200416'\n",
        "    resp = requests.post(url)\n",
        "    TPU_DRIVER_MODE = 1\n",
        "\n",
        "# The following is required to use TPU Driver as JAX's backend.\n",
        "from jax.config import config\n",
        "config.FLAGS.jax_xla_backend = \"tpu_driver\"\n",
        "config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']\n",
        "print(config.FLAGS.jax_backend_target)\n",
        "jax.devices()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8GX2u0gekPa"
      },
      "source": [
        "### Download enwik8 dataset and load data\n",
        "\n",
        "A standard script for enwik8 preprocessing is used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2euURrJBcmKX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6dc2b37-60cc-4faf-d786-752782861bdf"
      },
      "source": [
        "!wget --continue http://mattmahoney.net/dc/enwik8.zip\n",
        "!wget https://raw.githubusercontent.com/salesforce/awd-lstm-lm/master/data/enwik8/prep_enwik8.py\n",
        "!python3 prep_enwik8.py"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0QGM4-Atvkq"
      },
      "source": [
        "# The checkpoint was trained with python3.8 which uses pickle5, hence this hack.\n",
        "layers_base_path = '/usr/local/lib/python3.7/dist-packages/trax/layers/base.py'\n",
        "with open(layers_base_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "idx = lines.index('import pickle\\n')\n",
        "lines[idx] = 'import pickle5 as pickle\\n'\n",
        "with open(layers_base_path, 'w') as f:\n",
        "    f.writelines(lines)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKFdaeFuSwvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db9e3e66-29ca-43fe-ac6e-8f09b6963ff8"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "from trax.fastmath import numpy as jnp\n",
        "\n",
        "def raw_ds_to_tensor(raw_file_path):\n",
        "    with tf.io.gfile.GFile(raw_file_path, mode='rb') as f:\n",
        "        raw_data = f.read()\n",
        "        print(f'Bytes in {raw_file_path}:', len(raw_data))\n",
        "    return jnp.array(list(raw_data))\n",
        "\n",
        "testset_tensor, validset_tensor = map(raw_ds_to_tensor, [\n",
        "    '/content/test.txt.raw',\n",
        "    '/content/valid.txt.raw',\n",
        "])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ehl0E2tjhxy"
      },
      "source": [
        "### Download and load the trained checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF6OvI7W9Wbn"
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=18wrzKZLBtLuFOHwzuF-7i_p-rD2miE_6\n",
        "!tar -zxvf enwik8_checkpoint.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnFfDnQKqstR"
      },
      "source": [
        "import gin\n",
        "import trax\n",
        "\n",
        "MODEL_DIR = 'enwik8_checkpoint'\n",
        "\n",
        "gin.parse_config_file(f'./{MODEL_DIR}/config.gin')\n",
        "\n",
        "model = trax.models.HourglassLM(mode='eval')\n",
        "\n",
        "model.init_from_file(\n",
        "    f'./{MODEL_DIR}/model.pkl.gz',\n",
        "    weights_only=True\n",
        ")\n",
        "\n",
        "loss_fn = trax.layers.WeightedCategoryCrossEntropy()\n",
        "model_eval = trax.layers.Accelerate(trax.layers.Serial(\n",
        "    model,\n",
        "    loss_fn\n",
        "))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OuHGqx9vTVL"
      },
      "source": [
        "### Evaluate on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwg6GN3yKvgQ"
      },
      "source": [
        "from trax import fastmath\n",
        "from trax.fastmath import numpy as jnp\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def batched_inputs(data_gen, batch_size):\n",
        "  inp_stack, mask_stack = [], []\n",
        "\n",
        "  for input_example, mask in data_gen:\n",
        "    inp_stack.append(input_example)\n",
        "    mask_stack.append(mask)\n",
        "    if len(inp_stack) % batch_size == 0:\n",
        "      if len(set(len(example) for example in inp_stack)) > 1:\n",
        "        for x, m in zip(inp_stack, mask_stack):\n",
        "          yield x, m\n",
        "      else:\n",
        "        input_batch = jnp.stack(inp_stack)\n",
        "        mask_batch = jnp.stack(mask_stack)\n",
        "\n",
        "        yield input_batch, mask_batch\n",
        "      inp_stack, mask_stack = [], []\n",
        "\n",
        "  if len(inp_stack) > 0:\n",
        "    for x, m in zip(inp_stack, mask_stack):\n",
        "      yield x, m\n",
        "\n",
        "\n",
        "def run_full_evaluation(accelerated_model_with_loss, examples_data_gen,\n",
        "                        batch_size, pad_to_len=None):\n",
        "  # Important: we assume batch size per device = 1\n",
        "  assert batch_size % fastmath.local_device_count() == 0\n",
        "  assert fastmath.local_device_count() == 1 or \\\n",
        "         batch_size == fastmath.local_device_count()\n",
        "\n",
        "  loss_sum, n_tokens = 0.0, 0\n",
        "\n",
        "  def pad_right(inp_tensor):\n",
        "    if pad_to_len:\n",
        "      return jnp.pad(inp_tensor,\n",
        "                     [[0, 0], [0, max(0, pad_to_len - inp_tensor.shape[1])]])\n",
        "    else:\n",
        "      return inp_tensor\n",
        "\n",
        "  batch_gen = batched_inputs(examples_data_gen, batch_size)\n",
        "\n",
        "  def batch_leftover_example(input_example, example_mask):\n",
        "    def extend_shape_to_batch_size(tensor):\n",
        "      return jnp.repeat(tensor, repeats=batch_size, axis=0)\n",
        "\n",
        "    return map(extend_shape_to_batch_size,\n",
        "               (input_example[None, ...], example_mask[None, ...]))\n",
        "\n",
        "  for i, (inp, mask) in tqdm(enumerate(batch_gen)):\n",
        "    leftover_batch = False\n",
        "    # For leftover examples, we yield rank 1 tensors (unbatched) instead of\n",
        "    # rank 2 batches from our `batched_inputs` function. This convention allows\n",
        "    # a special behaviour for the leftover batches that have to be processed\n",
        "    # one by one.\n",
        "    if len(inp.shape) == 1:\n",
        "      inp, mask = batch_leftover_example(inp, mask)\n",
        "      leftover_batch = True\n",
        "\n",
        "    inp, mask = map(pad_right, [inp, mask])\n",
        "\n",
        "    example_losses = accelerated_model_with_loss((inp, inp, mask))\n",
        "\n",
        "    if leftover_batch:\n",
        "      example_losses = example_losses[:1]\n",
        "      mask = mask[:1]\n",
        "\n",
        "    example_lengths = mask.sum(axis=-1)\n",
        "\n",
        "    loss_sum += (example_lengths * example_losses).sum()\n",
        "    n_tokens += mask.sum()\n",
        "\n",
        "    if i % 200 == 0:\n",
        "      print(f'Batches: {i}, current loss: {loss_sum / float(n_tokens)}')\n",
        "\n",
        "  return loss_sum / float(n_tokens)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8B4xaZbaaxN"
      },
      "source": [
        "We evaluate chunks of length $128$ bytes, preceded by a context of $128 \\cdot 53$ bytes (total context length is $6912$)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8xAbG6PZfDm",
        "outputId": "19c18fc9-ffd9-4b8d-cf24-0b7f9ae3ab0e"
      },
      "source": [
        "# Prepare the input generator: it should yield (input, mask) tuples\n",
        "def contextful_eval_data(bytes_tensor, CHUNK_LEN, N_CHUNKS_BEFORE):\n",
        "    for start in range(0, len(bytes_tensor), CHUNK_LEN):\n",
        "        shifted_chunk = bytes_tensor[max(0, start - (N_CHUNKS_BEFORE * CHUNK_LEN)):\n",
        "                                                    start+CHUNK_LEN]\n",
        "        mask = jnp.zeros_like(shifted_chunk)\n",
        "        masked_len = min(CHUNK_LEN, len(bytes_tensor) - start)\n",
        "\n",
        "        mask = fastmath.index_update(mask, np.s_[-masked_len:], 1)\n",
        "\n",
        "        shifted_chunk = trax.data.inputs._pad_to_multiple_of(shifted_chunk,\n",
        "                                                             CHUNK_LEN, axis=0)\n",
        "        mask = trax.data.inputs._pad_to_multiple_of(mask, CHUNK_LEN, axis=0)\n",
        "\n",
        "        yield shifted_chunk, mask\n",
        "\n",
        "# Split the input into chunks of 6912\n",
        "PAD_TO_LEN = 6912 # We need to pad because shorten factor 3 is used.\n",
        "CHUNK_LEN = 128 #\n",
        "N_CHUNKS_BEFORE = 53\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "test_data_gen = contextful_eval_data(testset_tensor, CHUNK_LEN, N_CHUNKS_BEFORE)\n",
        "\n",
        "loss = run_full_evaluation(model_eval, test_data_gen, BATCH_SIZE, PAD_TO_LEN)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_GynuimYNxl",
        "outputId": "b289df04-99d2-4ed9-e17a-118c20639bca"
      },
      "source": [
        "print(f'Final perplexity: {loss}, final bpd: {loss / jnp.log(2)}')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DvYUpo4Xji5"
      },
      "source": [
        "### Generate text from the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YMco8prXiXn"
      },
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def autoregressive_sample(model, temp=1.0, batch_size=8, l=3072, vocab_size=256):\n",
        "  model = trax.layers.Accelerate(model)\n",
        "  x = np.zeros((batch_size, l), dtype=np.int32)\n",
        "\n",
        "  logits_prev = np.zeros((batch_size, l, vocab_size), dtype=np.float32)\n",
        "  for i in tqdm(range(l)):\n",
        "    logits = model(x)\n",
        "    np.testing.assert_array_almost_equal(logits_prev[:, :i], logits[:, :i])\n",
        "    logits_prev = logits\n",
        "\n",
        "    sample = trax.layers.logsoftmax_sample(logits[:, i, :], temperature=temp)\n",
        "    x[:, i] = sample\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPg_NysgamHO",
        "outputId": "b788ad88-f121-4d2d-d2a6-15f99f63cd6b"
      },
      "source": [
        "samples = autoregressive_sample(model, l=1026)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4p74ayeCjE_"
      },
      "source": [
        "Text sample generated by the model (unconditional generation - without any prompts):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "Hwtw9vXNc7h6",
        "outputId": "759165a1-ee58-4f51-887e-f27ee551a51e"
      },
      "source": [
        "bytes((samples[0]).tolist()).decode()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}