{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hourglass_enwik8.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdO8Wj1U5tLN"
      },
      "source": [
        "#### Copyright 2021 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRaMWbn1BMtI"
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\")\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmkqZ5M69BJS"
      },
      "source": [
        "# Hourglass: enwik8 evaluation [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/trax/blob/master/trax/models/research/examples/hourglass_enwik8.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vlpvzGh-O2N"
      },
      "source": [
        "This notebook was designed to run on TPU.\n",
        "\n",
        "To use TPUs in Colab, click \"Runtime\" on the main menu bar and select Change runtime type. Set \"TPU\" as the hardware accelerator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXBTKGSsegOy"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaqpNaQ4ShRP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff3fac8d-88c8-42ae-cffe-a0aa5a51ab7f"
      },
      "source": [
        "TRAX_GITHUB_URL = 'git+https://github.com/google/trax.git'\n",
        "!pip install -q --upgrade jax==0.2.21\n",
        "!pip install -q --upgrade jaxlib==0.1.71+cuda111 -f https://storage.googleapis.com/jax-releases/jax_releases.html\n",
        "!pip install -q $TRAX_GITHUB_URL\n",
        "!pip install -q pickle5\n",
        "!pip install -q neptune-client\n",
        "!pip install -q gin"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.4 MB 5.1 MB/s \n",
            "\u001b[?25h  Building wheel for trax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 132 kB 5.2 MB/s \n",
            "\u001b[?25h  Building wheel for pickle5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 275 kB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 829 kB 41.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 180 kB 55.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 131 kB 62.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 8.0 MB 37.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 79 kB 6.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 138 kB 56.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 47.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 129 kB 47.2 MB/s \n",
            "\u001b[?25h  Building wheel for neptune-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for strict-rfc3339 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "  Building wheel for gin (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVN8e_m9kg3K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b9f1d4f-a9db-4219-a196-3a8802d993ca"
      },
      "source": [
        "# Execute this for a proper TPU setup!\n",
        "# Make sure the Colab Runtime is set to Accelerator: TPU.\n",
        "import jax\n",
        "import requests\n",
        "import os\n",
        "if 'TPU_DRIVER_MODE' not in globals():\n",
        "    url = 'http://' + os.environ['COLAB_TPU_ADDR'].split(':')[0] + ':8475/requestversion/tpu_driver0.1-dev20200416'\n",
        "    resp = requests.post(url)\n",
        "    TPU_DRIVER_MODE = 1\n",
        "\n",
        "# The following is required to use TPU Driver as JAX's backend.\n",
        "from jax.config import config\n",
        "config.FLAGS.jax_xla_backend = \"tpu_driver\"\n",
        "config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']\n",
        "print(config.FLAGS.jax_backend_target)\n",
        "jax.devices()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grpc://10.55.87.106:8470\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
              " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
              " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
              " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
              " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
              " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
              " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
              " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8GX2u0gekPa"
      },
      "source": [
        "### Download enwik8 dataset and load data\n",
        "\n",
        "A standard script for enwik8 preprocessing is used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2euURrJBcmKX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6dc2b37-60cc-4faf-d786-752782861bdf"
      },
      "source": [
        "!wget --continue http://mattmahoney.net/dc/enwik8.zip\n",
        "!wget https://raw.githubusercontent.com/salesforce/awd-lstm-lm/master/data/enwik8/prep_enwik8.py\n",
        "!python3 prep_enwik8.py"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-07 05:41:17--  http://mattmahoney.net/dc/enwik8.zip\n",
            "Resolving mattmahoney.net (mattmahoney.net)... 67.195.197.24\n",
            "Connecting to mattmahoney.net (mattmahoney.net)|67.195.197.24|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 36445475 (35M) [application/zip]\n",
            "Saving to: ‘enwik8.zip’\n",
            "\n",
            "enwik8.zip          100%[===================>]  34.76M  2.00MB/s    in 17s     \n",
            "\n",
            "2021-10-07 05:41:34 (2.00 MB/s) - ‘enwik8.zip’ saved [36445475/36445475]\n",
            "\n",
            "--2021-10-07 05:41:34--  https://raw.githubusercontent.com/salesforce/awd-lstm-lm/master/data/enwik8/prep_enwik8.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 818 [text/plain]\n",
            "Saving to: ‘prep_enwik8.py.1’\n",
            "\n",
            "prep_enwik8.py.1    100%[===================>]     818  --.-KB/s    in 0s      \n",
            "\n",
            "2021-10-07 05:41:34 (29.6 MB/s) - ‘prep_enwik8.py.1’ saved [818/818]\n",
            "\n",
            "Length of enwik8: 100000000\n",
            "train.txt will have 90000000 bytes\n",
            "- Tokenizing...\n",
            "- Writing...\n",
            "valid.txt will have 5000000 bytes\n",
            "- Tokenizing...\n",
            "- Writing...\n",
            "test.txt will have 5000000 bytes\n",
            "- Tokenizing...\n",
            "- Writing...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0QGM4-Atvkq"
      },
      "source": [
        "# The checkpoint was trained with python3.8 which uses pickle5, hence this hack.\n",
        "layers_base_path = '/usr/local/lib/python3.7/dist-packages/trax/layers/base.py'\n",
        "with open(layers_base_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "idx = lines.index('import pickle\\n')\n",
        "lines[idx] = 'import pickle5 as pickle\\n'\n",
        "with open(layers_base_path, 'w') as f:\n",
        "    f.writelines(lines)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKFdaeFuSwvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db9e3e66-29ca-43fe-ac6e-8f09b6963ff8"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "from trax.fastmath import numpy as jnp\n",
        "\n",
        "def raw_ds_to_tensor(raw_file_path):\n",
        "    with tf.io.gfile.GFile(raw_file_path, mode='rb') as f:\n",
        "        raw_data = f.read()\n",
        "        print(f'Bytes in {raw_file_path}:', len(raw_data))\n",
        "    return jnp.array(list(raw_data))\n",
        "\n",
        "testset_tensor, validset_tensor = map(raw_ds_to_tensor, [\n",
        "    '/content/test.txt.raw',\n",
        "    '/content/valid.txt.raw',\n",
        "])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bytes in /content/test.txt.raw: 5000000\n",
            "Bytes in /content/valid.txt.raw: 5000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ehl0E2tjhxy"
      },
      "source": [
        "### Download and load the trained checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF6OvI7W9Wbn"
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=18wrzKZLBtLuFOHwzuF-7i_p-rD2miE_6\n",
        "!tar -zxvf enwik8_checkpoint.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnFfDnQKqstR"
      },
      "source": [
        "import gin\n",
        "import trax\n",
        "\n",
        "MODEL_DIR = 'enwik8_checkpoint'\n",
        "\n",
        "gin.parse_config_file(f'./{MODEL_DIR}/config.gin')\n",
        "\n",
        "model = trax.models.HourglassLM(mode='eval')\n",
        "\n",
        "model.init_from_file(\n",
        "    f'./{MODEL_DIR}/model.pkl.gz',\n",
        "    weights_only=True\n",
        ")\n",
        "\n",
        "loss_fn = trax.layers.WeightedCategoryCrossEntropy()\n",
        "model_eval = trax.layers.Accelerate(trax.layers.Serial(\n",
        "    model,\n",
        "    loss_fn\n",
        "))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OuHGqx9vTVL"
      },
      "source": [
        "### Evaluate on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwg6GN3yKvgQ"
      },
      "source": [
        "from trax import fastmath\n",
        "from trax.fastmath import numpy as jnp\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def batched_inputs(data_gen, batch_size):\n",
        "  inp_stack, mask_stack = [], []\n",
        "\n",
        "  for input_example, mask in data_gen:\n",
        "    inp_stack.append(input_example)\n",
        "    mask_stack.append(mask)\n",
        "    if len(inp_stack) % batch_size == 0:\n",
        "      if len(set(len(example) for example in inp_stack)) > 1:\n",
        "        for x, m in zip(inp_stack, mask_stack):\n",
        "          yield x, m\n",
        "      else:\n",
        "        input_batch = jnp.stack(inp_stack)\n",
        "        mask_batch = jnp.stack(mask_stack)\n",
        "\n",
        "        yield input_batch, mask_batch\n",
        "      inp_stack, mask_stack = [], []\n",
        "\n",
        "  if len(inp_stack) > 0:\n",
        "    for x, m in zip(inp_stack, mask_stack):\n",
        "      yield x, m\n",
        "\n",
        "\n",
        "def run_full_evaluation(accelerated_model_with_loss, examples_data_gen,\n",
        "                        batch_size, pad_to_len=None):\n",
        "  # Important: we assume batch size per device = 1\n",
        "  assert batch_size % fastmath.local_device_count() == 0\n",
        "  assert fastmath.local_device_count() == 1 or \\\n",
        "         batch_size == fastmath.local_device_count()\n",
        "\n",
        "  loss_sum, n_tokens = 0.0, 0\n",
        "\n",
        "  def pad_right(inp_tensor):\n",
        "    if pad_to_len:\n",
        "      return jnp.pad(inp_tensor,\n",
        "                     [[0, 0], [0, max(0, pad_to_len - inp_tensor.shape[1])]])\n",
        "    else:\n",
        "      return inp_tensor\n",
        "\n",
        "  batch_gen = batched_inputs(examples_data_gen, batch_size)\n",
        "\n",
        "  def batch_leftover_example(input_example, example_mask):\n",
        "    def extend_shape_to_batch_size(tensor):\n",
        "      return jnp.repeat(tensor, repeats=batch_size, axis=0)\n",
        "\n",
        "    return map(extend_shape_to_batch_size,\n",
        "               (input_example[None, ...], example_mask[None, ...]))\n",
        "\n",
        "  for i, (inp, mask) in tqdm(enumerate(batch_gen)):\n",
        "    leftover_batch = False\n",
        "    # For leftover examples, we yield rank 1 tensors (unbatched) instead of\n",
        "    # rank 2 batches from our `batched_inputs` function. This convention allows\n",
        "    # a special behaviour for the leftover batches that have to be processed\n",
        "    # one by one.\n",
        "    if len(inp.shape) == 1:\n",
        "      inp, mask = batch_leftover_example(inp, mask)\n",
        "      leftover_batch = True\n",
        "\n",
        "    inp, mask = map(pad_right, [inp, mask])\n",
        "\n",
        "    example_losses = accelerated_model_with_loss((inp, inp, mask))\n",
        "\n",
        "    if leftover_batch:\n",
        "      example_losses = example_losses[:1]\n",
        "      mask = mask[:1]\n",
        "\n",
        "    example_lengths = mask.sum(axis=-1)\n",
        "\n",
        "    loss_sum += (example_lengths * example_losses).sum()\n",
        "    n_tokens += mask.sum()\n",
        "\n",
        "    if i % 200 == 0:\n",
        "      print(f'Batches: {i}, current loss: {loss_sum / float(n_tokens)}')\n",
        "\n",
        "  return loss_sum / float(n_tokens)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8B4xaZbaaxN"
      },
      "source": [
        "We evaluate chunks of length $128$ bytes, preceded by a context of $128 \\cdot 53$ bytes (total context length is $6912$)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8xAbG6PZfDm",
        "outputId": "19c18fc9-ffd9-4b8d-cf24-0b7f9ae3ab0e"
      },
      "source": [
        "# Prepare the input generator: it should yield (input, mask) tuples\n",
        "def contextful_eval_data(bytes_tensor, CHUNK_LEN, N_CHUNKS_BEFORE):\n",
        "    for start in range(0, len(bytes_tensor), CHUNK_LEN):\n",
        "        shifted_chunk = bytes_tensor[max(0, start - (N_CHUNKS_BEFORE * CHUNK_LEN)):\n",
        "                                                    start+CHUNK_LEN]\n",
        "        mask = jnp.zeros_like(shifted_chunk)\n",
        "        masked_len = min(CHUNK_LEN, len(bytes_tensor) - start)\n",
        "\n",
        "        mask = fastmath.index_update(mask, jax.ops.index[-masked_len:], 1)\n",
        "\n",
        "        shifted_chunk = trax.data.inputs._pad_to_multiple_of(shifted_chunk,\n",
        "                                                             CHUNK_LEN, axis=0)\n",
        "        mask = trax.data.inputs._pad_to_multiple_of(mask, CHUNK_LEN, axis=0)\n",
        "\n",
        "        yield shifted_chunk, mask\n",
        "\n",
        "# Split the input into chunks of 6912\n",
        "PAD_TO_LEN = 6912 # We need to pad because shorten factor 3 is used.\n",
        "CHUNK_LEN = 128 #\n",
        "N_CHUNKS_BEFORE = 53\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "test_data_gen = contextful_eval_data(testset_tensor, CHUNK_LEN, N_CHUNKS_BEFORE)\n",
        "\n",
        "loss = run_full_evaluation(model_eval, test_data_gen, BATCH_SIZE, PAD_TO_LEN)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [08:27, 507.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batches: 0, current loss: 1.1698029041290283\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "201it [15:34,  2.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batches: 200, current loss: 0.6762865781784058\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "401it [22:42,  2.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batches: 400, current loss: 0.6353754997253418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "601it [29:47,  2.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batches: 600, current loss: 0.6671227812767029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "801it [36:53,  2.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batches: 800, current loss: 0.6871113777160645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1001it [44:01,  2.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batches: 1000, current loss: 0.7012861967086792\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1201it [51:08,  2.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batches: 1200, current loss: 0.7057864665985107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1401it [58:13,  2.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batches: 1400, current loss: 0.7055128216743469\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1601it [1:05:19,  2.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batches: 1600, current loss: 0.7026289701461792\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1801it [1:12:27,  2.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batches: 1800, current loss: 0.6966000199317932\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2001it [1:19:36,  2.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batches: 2000, current loss: 0.6651851534843445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2201it [1:26:43,  2.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batches: 2200, current loss: 0.6696659922599792\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2401it [1:33:49,  2.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batches: 2400, current loss: 0.6754175424575806\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2601it [1:40:55,  2.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batches: 2600, current loss: 0.6817601919174194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2801it [1:48:03,  2.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batches: 2800, current loss: 0.6837883591651917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3001it [1:55:13,  2.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batches: 3000, current loss: 0.6857511401176453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3201it [2:02:22,  2.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batches: 3200, current loss: 0.6849543452262878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3401it [2:09:32,  2.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batches: 3400, current loss: 0.6871944665908813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3601it [2:16:40,  2.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batches: 3600, current loss: 0.6876044273376465\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3801it [2:23:47,  2.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batches: 3800, current loss: 0.690453052520752\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4001it [2:30:53,  2.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batches: 4000, current loss: 0.689698338508606\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4201it [2:37:59,  2.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batches: 4200, current loss: 0.6900818943977356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4401it [2:45:05,  2.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batches: 4400, current loss: 0.6902956366539001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4601it [2:52:15,  2.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batches: 4600, current loss: 0.689020037651062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4801it [2:59:22,  2.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batches: 4800, current loss: 0.6893221735954285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4938it [3:04:14,  2.24s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_GynuimYNxl",
        "outputId": "b289df04-99d2-4ed9-e17a-118c20639bca"
      },
      "source": [
        "print(f'Final perplexity: {loss}, final bpd: {loss / jnp.log(2)}')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final perplexity: 0.6912750005722046, final bpd: 0.997299075126648\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DvYUpo4Xji5"
      },
      "source": [
        "### Generate text from the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YMco8prXiXn"
      },
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def autoregressive_sample(model, temp=1.0, batch_size=8, l=3072, vocab_size=256):\n",
        "  model = trax.layers.Accelerate(model)\n",
        "  x = np.zeros((batch_size, l), dtype=np.int32)\n",
        "\n",
        "  logits_prev = np.zeros((batch_size, l, vocab_size), dtype=np.float32)\n",
        "  for i in tqdm(range(l)):\n",
        "    logits = model(x)\n",
        "    np.testing.assert_array_almost_equal(logits_prev[:, :i], logits[:, :i])\n",
        "    logits_prev = logits\n",
        "\n",
        "    sample = trax.layers.logsoftmax_sample(logits[:, i, :], temperature=temp)\n",
        "    x[:, i] = sample\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPg_NysgamHO",
        "outputId": "b788ad88-f121-4d2d-d2a6-15f99f63cd6b"
      },
      "source": [
        "samples = autoregressive_sample(model, l=1026)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 513/513 [05:47<00:00,  1.48it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4p74ayeCjE_"
      },
      "source": [
        "Text sample generated by the model (unconditional generation - without any prompts):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "Hwtw9vXNc7h6",
        "outputId": "759165a1-ee58-4f51-887e-f27ee551a51e"
      },
      "source": [
        "bytes((samples[0]).tolist()).decode()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' the political laws.  War also helped develop the [[Soviet Union|Soviet]] system in western Europe, as did [[Luxembourg]] and the Church of [[Sweden]]. The immediate impact of nuclear war took place in eastern Europe, bankrupted by serious [[Nuclear fallout|fallout]] from [[Early Modern Europe|miners from both sides]]. The state ally strategic eastern Europe had immediately concluded the war in both sides, although it was extremely weak. Meanwhile, the US was developing an urban and commercial life of more t'"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    }
  ]
}