# Copyright 2019 The Trax Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import trax.models
import trax.optimizers
import trax.supervised.trainer_lib
import trax.rl
import trax.rl.space_serializer
import trax.rl.trainers

# Parameters for BoxSpaceSerializer:
# ==============================================================================
BoxSpaceSerializer.precision = 2

# Parameters for MultifactorSchedule:
# ==============================================================================
world_model/MultifactorSchedule.constant = 1.0
world_model/MultifactorSchedule.factors = 'constant * linear_warmup * rsqrt_decay'
world_model/MultifactorSchedule.warmup_steps = 10000

# Parameters for Adafactor:
# ==============================================================================
Adafactor.learning_rate = 1e-2
Adafactor.beta1 = 0.0
Adafactor.decay_rate = 0.8
Adafactor.clipping_threshold = 1.0
Adafactor.epsilon1 = 1e-30
Adafactor.epsilon2 = 0.001
Adafactor.factored = True
Adafactor.multiply_by_parameter_scale = True

# Parameters for TransformerDecoder:
# ==============================================================================
TransformerDecoder.attention_type = @policy/trax.layers.TimeBinCausalAttention
TransformerDecoder.d_model = 256
TransformerDecoder.d_ff = 512
TransformerDecoder.n_layers = 3
TransformerDecoder.n_heads = 4
TransformerDecoder.dropout = 0.0
TransformerDecoder.max_len = 8192

# Parameters for PPO:
# ==============================================================================
PPO.n_optimizer_steps = 10
PPO.optimizer_batch_size = 16
PPO.target_kl = 0.1
PPO.boundary = 100
PPO.max_timestep = 100
PPO.max_timestep_eval = 100
PPO.random_seed = None
PPO.gamma = 1.0
PPO.lambda_ = 0.95
PPO.value_weight = 1.0
PPO.entropy_weight = 0.1
PPO.done_frac_for_policy_save = 0
PPO.len_history_for_policy = None
PPO.separate_eval = False
PPO.save_every_n = 1
PPO.policy_and_value_model = @trax.models.TransformerDecoder
PPO.policy_and_value_optimizer = @trax.optimizers.Adafactor
PPO.policy_and_value_vocab_size = 128
PPO.trajectory_dump_min_count_per_shard = 8
PPO.print_every_optimizer_steps = 1

## Parameters for TimeBinCausalAttention:
## ==============================================================================
world_model/TimeBinCausalAttention.dropout = 0.1
world_model/TimeBinCausalAttention.bin_length = 256

policy/TimeBinCausalAttention.dropout = 0.0
policy/TimeBinCausalAttention.bin_length = 256

# Parameters for SerializedSequenceSimulatedEnvProblem:
# ==============================================================================
SerializedSequenceSimulatedEnvProblem.model = @world_model/trax.models.TransformerLM
SerializedSequenceSimulatedEnvProblem.reward_fn = @trax.rl.onlinetune_reward_fn
SerializedSequenceSimulatedEnvProblem.done_fn = @trax.rl.onlinetune_done_fn
SerializedSequenceSimulatedEnvProblem.vocab_size = 128
SerializedSequenceSimulatedEnvProblem.max_trajectory_length = 101
SerializedSequenceSimulatedEnvProblem.significance_decay = 0.8

# Parameters for SimPLe:
# ==============================================================================
SimPLe.policy_trainer_class = @trax.rl.trainers.PPO
SimPLe.n_real_epochs = 1
SimPLe.n_model_initial_train_steps = 70000
SimPLe.n_model_train_steps_per_epoch = 10000
SimPLe.model_train_batch_size = 32
SimPLe.simulated_env_problem_class = @trax.rl.SerializedSequenceSimulatedEnvProblem
SimPLe.simulated_batch_size = 128
SimPLe.n_simulated_epochs = 50
SimPLe.initial_trajectory_mix_prob = 0.9
SimPLe.init_policy_from_world_model = True

# Parameters for TransformerLM:
# ==============================================================================
world_model/TransformerLM.attention_type = @world_model/trax.layers.TimeBinCausalAttention
world_model/TransformerLM.d_model = 256
world_model/TransformerLM.d_ff = 512
world_model/TransformerLM.n_layers = 3
world_model/TransformerLM.n_heads = 4
world_model/TransformerLM.dropout = 0.1
world_model/TransformerLM.max_len = 8192

# Parameters for train:
# ==============================================================================
world_model/train.eval_frequency = 1000
world_model/train.optimizer = @trax.optimizers.Adafactor
