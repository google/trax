{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/SauravMaheshkar/trax/blob/SauravMaheshkar-example-1/examples/Deep_N_Gram_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lAAzPCP8n05S"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "# Copyright 2020 Google LLC.\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcV2B-3LnvBk"
   },
   "source": [
    "Author - [@SauravMaheshkar](https://github.com/SauravMaheshkar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEg7rw6fnr0q",
    "papermill": {
     "duration": 0.024472,
     "end_time": "2020-10-19T05:23:45.163806",
     "exception": false,
     "start_time": "2020-10-19T05:23:45.139334",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Downloading the Trax Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iVotT-qnr0q",
    "papermill": {
     "duration": 0.024546,
     "end_time": "2020-10-19T05:23:45.211638",
     "exception": false,
     "start_time": "2020-10-19T05:23:45.187092",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "[Trax](https://trax-ml.readthedocs.io/en/latest/) is an end-to-end library for deep learning that focuses on clear code and speed. It is actively used and maintained in the [Google Brain team](https://research.google/teams/brain/). This notebook ([run it in colab](https://colab.research.google.com/github/google/trax/blob/master/trax/intro.ipynb)) shows how to use Trax and where you can find more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "id": "LTV7nHkWnr0q",
    "papermill": {
     "duration": 55.642763,
     "end_time": "2020-10-19T05:24:40.877583",
     "exception": false,
     "start_time": "2020-10-19T05:23:45.234820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# For example, if trax is inside a 'src' directory\n",
    "project_root = os.environ.get('TRAX_PROJECT_ROOT', '')\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "# Option to verify the import path\n",
    "print(f\"Python will look for packages in: {sys.path[0]}\")\n",
    "\n",
    "# Import trax\n",
    "import trax\n",
    "\n",
    "# Verify the source of the imported package\n",
    "print(f\"Imported trax from: {trax.__file__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4e-X6Ranr0s",
    "papermill": {
     "duration": 0.121469,
     "end_time": "2020-10-19T05:24:41.120599",
     "exception": false,
     "start_time": "2020-10-19T05:24:40.999130",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Importing Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaoHVZj0nr0s",
    "papermill": {
     "duration": 0.117117,
     "end_time": "2020-10-19T05:24:41.355694",
     "exception": false,
     "start_time": "2020-10-19T05:24:41.238577",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this notebook we will use the following packages:\n",
    "\n",
    "* [**Pandas**](https://pandas.pydata.org/) is a fast, powerful, flexible and easy to use open-source data analysis and manipulation tool, built on top of the Python programming language. It offers a fast and efficient DataFrame object for data manipulation with integrated indexing.\n",
    "* [**os**](https://docs.python.org/3/library/os.html) module provides a portable way of using operating system dependent functionality.\n",
    "* [**trax**](https://trax-ml.readthedocs.io/en/latest/trax.html) is an end-to-end library for deep learning that focuses on clear code and speed.\n",
    "* [**random**](https://docs.python.org/3/library/random.html) module implements pseudo-random number generators for various distributions.\n",
    "* [**itertools**](https://docs.python.org/3/library/itertools.html) module implements a number of iterator building blocks inspired by constructs from APL, Haskell, and SML. Each has been recast in a form suitable for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "h8vjYA-8nr0s",
    "papermill": {
     "duration": 13.181434,
     "end_time": "2020-10-19T05:24:54.656623",
     "exception": false,
     "start_time": "2020-10-19T05:24:41.475189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import trax.fastmath.numpy as np\n",
    "import random as rnd\n",
    "from trax import layers as tl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZaUGa2Lnr0s",
    "papermill": {
     "duration": 0.118759,
     "end_time": "2020-10-19T05:24:54.899617",
     "exception": false,
     "start_time": "2020-10-19T05:24:54.780858",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbwaTxIFnr0s",
    "papermill": {
     "duration": 0.122704,
     "end_time": "2020-10-19T05:24:55.144895",
     "exception": false,
     "start_time": "2020-10-19T05:24:55.022191",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For this project, I've used the [gothic-literature](https://www.kaggle.com/charlesaverill/gothic-literature), [shakespeare-plays](https://www.kaggle.com/kingburrito666/shakespeare-plays) and [shakespeareonline](https://www.kaggle.com/kewagbln/shakespeareonline) datasets from the Kaggle library.\n",
    "\n",
    "We perform the following steps for loading in the data:\n",
    "\n",
    "* Iterate over all the directories in the `/kaggle/input/` directory\n",
    "* Filter out `.txt` files\n",
    "* Make a `lines` list containing the individual lines from all the datasets combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a7mudKI5nr0s",
    "papermill": {
     "duration": 0.456359,
     "end_time": "2020-10-19T05:24:55.716572",
     "exception": false,
     "start_time": "2020-10-19T05:24:55.260213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import zipfile\n",
    "\n",
    "\n",
    "def download_datasets(download_dir):\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "    # Define the datasets with output filename and download URL\n",
    "    datasets = [\n",
    "        {\n",
    "            \"filename\": \"gothic-literature.zip\",\n",
    "            \"url\": \"https://www.kaggle.com/api/v1/datasets/download/charlesaverill/gothic-literature\"\n",
    "        },\n",
    "        {\n",
    "            \"filename\": \"shakespeare-plays.zip\",\n",
    "            \"url\": \"https://www.kaggle.com/api/v1/datasets/download/kingburrito666/shakespeare-plays\"\n",
    "        },\n",
    "        {\n",
    "            \"filename\": \"shakespeareonline.zip\",\n",
    "            \"url\": \"https://www.kaggle.com/api/v1/datasets/download/kewagbln/shakespeareonline\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Download each dataset using curl\n",
    "    for dataset in datasets:\n",
    "        output_path = os.path.join(download_dir, dataset[\"filename\"])\n",
    "        # Build the curl command (using -L for following redirects)\n",
    "        cmd = [\n",
    "            \"curl\",\n",
    "            \"-L\",\n",
    "            \"-o\", output_path,\n",
    "            dataset[\"url\"]\n",
    "        ]\n",
    "        print(f\"Downloading {dataset['filename']}...\")\n",
    "        subprocess.run(cmd, check=True)\n",
    "        print(f\"Downloaded to {output_path}\")\n",
    "\n",
    "\n",
    "def extract_zip_files(download_dir, extract_dir):\n",
    "    os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "    # Iterate through the zip files in the download directory\n",
    "    for file in os.listdir(download_dir):\n",
    "        if file.lower().endswith(\".zip\"):\n",
    "            zip_path = os.path.join(download_dir, file)\n",
    "            # Create a subdirectory for each zip file (optional)\n",
    "            extract_subdir = os.path.join(extract_dir, os.path.splitext(file)[0])\n",
    "            os.makedirs(extract_subdir, exist_ok=True)\n",
    "            print(f\"Extracting {zip_path} to {extract_subdir}...\")\n",
    "            with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "                z.extractall(extract_subdir)\n",
    "            print(\"Extraction completed.\")\n",
    "\n",
    "\n",
    "def read_text_files(extracted_dir):\n",
    "    lines = []\n",
    "\n",
    "    # Walk through the unzipped directories and process each .txt file\n",
    "    for root, _, files in os.walk(extracted_dir):\n",
    "        for filename in files:\n",
    "            if filename.lower().endswith(\".txt\"):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                print(f\"Reading {file_path}...\")\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    for line in f:\n",
    "                        processed_line = line.strip()\n",
    "                        if processed_line:\n",
    "                            lines.append(processed_line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set download and extraction directories\n",
    "download_dir = os.path.expanduser(\"~/Downloads\")\n",
    "extract_dir = os.path.join(download_dir, \"extracted_datasets\")\n",
    "\n",
    "# Download datasets using curl\n",
    "download_datasets(download_dir)\n",
    "\n",
    "# Extract downloaded zip files\n",
    "extract_zip_files(download_dir, extract_dir)\n",
    "\n",
    "# Read text files from extracted data\n",
    "all_lines = read_text_files(extract_dir)\n",
    "\n",
    "print(f\"Total non-empty lines read: {len(all_lines)}\")\n",
    "# For example purposes, printing first 10 lines\n",
    "print(\"\\nFirst 10 lines:\")\n",
    "for line in all_lines[:10]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPifypFdnr0s",
    "papermill": {
     "duration": 0.113664,
     "end_time": "2020-10-19T05:24:55.951966",
     "exception": false,
     "start_time": "2020-10-19T05:24:55.838302",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eU58tWP3nr0s",
    "papermill": {
     "duration": 0.119888,
     "end_time": "2020-10-19T05:24:56.194726",
     "exception": false,
     "start_time": "2020-10-19T05:24:56.074838",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Converting to Lowercase\n",
    "\n",
    "Converting all the characters in the `lines` list to **lowercase**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAxU3uzunr0s",
    "papermill": {
     "duration": 0.253923,
     "end_time": "2020-10-19T05:24:56.569875",
     "exception": false,
     "start_time": "2020-10-19T05:24:56.315952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, line in enumerate(all_lines):\n",
    "    all_lines[i] = line.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voNUJBrRnr0s",
    "papermill": {
     "duration": 0.11122,
     "end_time": "2020-10-19T05:24:56.795120",
     "exception": false,
     "start_time": "2020-10-19T05:24:56.683900",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Converting into Tensors\n",
    "\n",
    "Creating a function to convert each line into a tensor by converting each character into it's ASCII value. And adding a optional `EOS`(**End of statement**) character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0F2sUJfnr0s",
    "papermill": {
     "duration": 0.131432,
     "end_time": "2020-10-19T05:24:57.037392",
     "exception": false,
     "start_time": "2020-10-19T05:24:56.905960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def line_to_tensor(line, EOS_int=1):\n",
    "    tensor = []\n",
    "    for c in line:\n",
    "        c_int = ord(c)\n",
    "        tensor.append(c_int)\n",
    "\n",
    "    tensor.append(EOS_int)\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYT5__Danr0s",
    "papermill": {
     "duration": 0.109763,
     "end_time": "2020-10-19T05:24:57.259043",
     "exception": false,
     "start_time": "2020-10-19T05:24:57.149280",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Creating a Batch Generator\n",
    "\n",
    "Here, we create a `batch_generator()` function to yield a batch and mask generator. We perform the following steps:\n",
    "\n",
    "* Shuffle the lines if not shuffled\n",
    "* Convert the lines into a Tensor\n",
    "* Pad the lines if it's less than the maximum length\n",
    "* Generate a mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-D_5L_snr0s",
    "papermill": {
     "duration": 0.134497,
     "end_time": "2020-10-19T05:24:57.503870",
     "exception": false,
     "start_time": "2020-10-19T05:24:57.369373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_generator(batch_size, max_length, data_lines, line_to_tensor=line_to_tensor, shuffle=True):\n",
    "    index = 0\n",
    "    cur_batch = []\n",
    "    num_lines = len(data_lines)\n",
    "    lines_index = [*range(num_lines)]\n",
    "\n",
    "    if shuffle:\n",
    "        rnd.shuffle(lines_index)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        if index >= num_lines:\n",
    "            index = 0\n",
    "            if shuffle:\n",
    "                rnd.shuffle(lines_index)\n",
    "\n",
    "        line = data_lines[lines_index[index]]\n",
    "\n",
    "        if len(line) < max_length:\n",
    "            cur_batch.append(line)\n",
    "\n",
    "        index += 1\n",
    "\n",
    "        if len(cur_batch) == batch_size:\n",
    "\n",
    "            batch = []\n",
    "            mask = []\n",
    "\n",
    "            for li in cur_batch:\n",
    "                tensor = line_to_tensor(li)\n",
    "\n",
    "                pad = [0] * (max_length - len(tensor))\n",
    "                tensor_pad = tensor + pad\n",
    "                batch.append(tensor_pad)\n",
    "\n",
    "                example_mask = [0 if t == 0 else 1 for t in tensor_pad]\n",
    "                mask.append(example_mask)\n",
    "\n",
    "            batch_np_arr = np.array(batch)\n",
    "            mask_np_arr = np.array(mask)\n",
    "\n",
    "            yield batch_np_arr, batch_np_arr, mask_np_arr\n",
    "\n",
    "            cur_batch = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = data_generator(2, 10, all_lines, line_to_tensor=line_to_tensor, shuffle=True)\n",
    "print(next(generator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biglhqPjnr0s",
    "papermill": {
     "duration": 0.113922,
     "end_time": "2020-10-19T05:24:57.728762",
     "exception": false,
     "start_time": "2020-10-19T05:24:57.614840",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Defining the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JgMdnTonr0s",
    "papermill": {
     "duration": 0.110544,
     "end_time": "2020-10-19T05:24:57.950897",
     "exception": false,
     "start_time": "2020-10-19T05:24:57.840353",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Gated Recurrent Unit\n",
    "\n",
    "This function generates a GRU Language Model, consisting of the following layers:\n",
    "\n",
    "* ShiftRight()\n",
    "* Embedding()\n",
    "* GRU Units(Number specified by the `n_layers` parameter)\n",
    "* Dense() Layer\n",
    "* LogSoftmax() Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MSA3bpCHnr0s",
    "papermill": {
     "duration": 0.124594,
     "end_time": "2020-10-19T05:24:58.186525",
     "exception": false,
     "start_time": "2020-10-19T05:24:58.061931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def GRULM(vocab_size=256, d_model=512, n_layers=2, mode='train'):\n",
    "    model = tl.Serial(\n",
    "        tl.ShiftRight(mode=mode),\n",
    "        tl.Embedding(vocab_size=vocab_size, d_feature=d_model),\n",
    "        [tl.GRU(n_units=d_model) for _ in range(n_layers)],\n",
    "        tl.Dense(n_units=vocab_size),\n",
    "        tl.LogSoftmax()\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9A0JtfgCnr0s",
    "papermill": {
     "duration": 0.150132,
     "end_time": "2020-10-19T05:24:58.463252",
     "exception": false,
     "start_time": "2020-10-19T05:24:58.313120",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Long Short Term Memory\n",
    "\n",
    "This function generates a LSTM Language Model, consisting of the following layers:\n",
    "\n",
    "* ShiftRight()\n",
    "* Embedding()\n",
    "* LSTM Units(Number specified by the `n_layers` parameter)\n",
    "* Dense() Layer\n",
    "* LogSoftmax() Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ScuXPmvLnr0s",
    "papermill": {
     "duration": 0.129976,
     "end_time": "2020-10-19T05:24:58.717410",
     "exception": false,
     "start_time": "2020-10-19T05:24:58.587434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def LSTMLM(vocab_size=256, d_model=512, n_layers=2, mode='train'):\n",
    "    model = tl.Serial(\n",
    "        tl.ShiftRight(mode=mode),\n",
    "        tl.Embedding(vocab_size=vocab_size, d_feature=d_model),\n",
    "        [tl.LSTM(n_units=d_model) for _ in range(n_layers)],\n",
    "        tl.Dense(n_units=vocab_size),\n",
    "        tl.LogSoftmax()\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWVaUwG1nr0s",
    "papermill": {
     "duration": 0.130305,
     "end_time": "2020-10-19T05:24:58.971978",
     "exception": false,
     "start_time": "2020-10-19T05:24:58.841673",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Simple Recurrent Unit\n",
    "\n",
    "This function generates a SRU Language Model, consisting of the following layers:\n",
    "\n",
    "* ShiftRight()\n",
    "* Embedding()\n",
    "* SRU Units(Number specified by the `n_layers` parameter)\n",
    "* Dense() Layer\n",
    "* LogSoftmax() Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECzZRknPnr0s",
    "papermill": {
     "duration": 0.12795,
     "end_time": "2020-10-19T05:24:59.221979",
     "exception": false,
     "start_time": "2020-10-19T05:24:59.094029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SRULM(vocab_size=256, d_model=512, n_layers=2, mode='train'):\n",
    "    model = tl.Serial(\n",
    "        tl.ShiftRight(mode=mode),\n",
    "        tl.Embedding(vocab_size=vocab_size, d_feature=d_model),\n",
    "        [tl.SRU(n_units=d_model) for _ in range(n_layers)],\n",
    "        tl.Dense(n_units=vocab_size),\n",
    "        tl.LogSoftmax()\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1i8UlSvhnr0s",
    "outputId": "f4894449-5399-48c8-e22d-a8fa05be3615",
    "papermill": {
     "duration": 0.132413,
     "end_time": "2020-10-19T05:24:59.466681",
     "exception": false,
     "start_time": "2020-10-19T05:24:59.334268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "GRUmodel = GRULM(n_layers=5)\n",
    "LSTMmodel = LSTMLM(n_layers=5)\n",
    "SRUmodel = SRULM(n_layers=5)\n",
    "print(GRUmodel)\n",
    "print(LSTMmodel)\n",
    "print(SRUmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "As2O2Zj8nr0t",
    "papermill": {
     "duration": 0.117255,
     "end_time": "2020-10-19T05:24:59.712882",
     "exception": false,
     "start_time": "2020-10-19T05:24:59.595627",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxIs1y_Gnr0t",
    "papermill": {
     "duration": 0.113458,
     "end_time": "2020-10-19T05:24:59.939569",
     "exception": false,
     "start_time": "2020-10-19T05:24:59.826111",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here, we declare `the batch_size` and the `max_length` hyperparameters for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BLKz_gfKnr0t",
    "papermill": {
     "duration": 0.121757,
     "end_time": "2020-10-19T05:25:00.176474",
     "exception": false,
     "start_time": "2020-10-19T05:25:00.054717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "max_length = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUKNlXAmnr0t",
    "papermill": {
     "duration": 0.111425,
     "end_time": "2020-10-19T05:25:00.399880",
     "exception": false,
     "start_time": "2020-10-19T05:25:00.288455",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Creating Evaluation and Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TYJepc9Knr0t",
    "papermill": {
     "duration": 0.130539,
     "end_time": "2020-10-19T05:25:00.641885",
     "exception": false,
     "start_time": "2020-10-19T05:25:00.511346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_lines = all_lines[-1000:]  # Create a holdout validation set\n",
    "lines = all_lines[:-1000]  # Leave the rest for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DbI1fFSnr0t",
    "papermill": {
     "duration": 0.112994,
     "end_time": "2020-10-19T05:25:00.871007",
     "exception": false,
     "start_time": "2020-10-19T05:25:00.758013",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LKJoIzenr0t",
    "papermill": {
     "duration": 0.112218,
     "end_time": "2020-10-19T05:25:01.096544",
     "exception": false,
     "start_time": "2020-10-19T05:25:00.984326",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here, we create a function to train the models. This function does the following:\n",
    "\n",
    "* Creating a Train and Evaluation Generator that cycles infinetely using the `itertools` module\n",
    "* Train the Model using Adam Optimizer\n",
    "* Use the Accuracy Metric for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4-fSW3Tnr0t",
    "papermill": {
     "duration": 0.130503,
     "end_time": "2020-10-19T05:25:01.339549",
     "exception": false,
     "start_time": "2020-10-19T05:25:01.209046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trax.learning.supervised import training\n",
    "from trax import optimizers as optimizers\n",
    "import itertools\n",
    "\n",
    "\n",
    "def train_model(model, data_generator, batch_size=32, max_length=64, lines=lines, eval_lines=eval_lines, n_steps=10,\n",
    "                output_dir='model/'):\n",
    "    bare_train_generator = data_generator(batch_size, max_length, data_lines=lines)\n",
    "    infinite_train_generator = itertools.cycle(bare_train_generator)\n",
    "\n",
    "    bare_eval_generator = data_generator(batch_size, max_length, data_lines=eval_lines)\n",
    "    infinite_eval_generator = itertools.cycle(bare_eval_generator)\n",
    "\n",
    "    train_task = training.TrainTask(\n",
    "        labeled_data=infinite_train_generator,\n",
    "        loss_layer=tl.CrossEntropyLoss(),\n",
    "        optimizer=optimizers.Adam(0.0005),\n",
    "        n_steps_per_checkpoint=1\n",
    "    )\n",
    "\n",
    "    eval_task = training.EvalTask(\n",
    "        labeled_data=infinite_eval_generator,\n",
    "        metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n",
    "        n_eval_batches=1\n",
    "    )\n",
    "\n",
    "    training_loop = training.Loop(model,\n",
    "                                  train_task,\n",
    "                                  eval_tasks=[eval_task],\n",
    "                                  output_dir=output_dir\n",
    "                                  )\n",
    "\n",
    "    training_loop.run(n_steps=n_steps)\n",
    "\n",
    "    return training_loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dykzx2t1nr0t",
    "papermill": {
     "duration": 79.597768,
     "end_time": "2020-10-19T05:26:21.064134",
     "exception": false,
     "start_time": "2020-10-19T05:25:01.466366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "shutil.rmtree(os.path.expanduser('model/GRU'), ignore_errors=True)\n",
    "GRU_training_loop = train_model(GRUmodel, data_generator, n_steps=10, output_dir='model/GRU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4w9jvGYDnr0t",
    "papermill": {
     "duration": 93.801876,
     "end_time": "2020-10-19T05:27:55.049974",
     "exception": false,
     "start_time": "2020-10-19T05:26:21.248098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "shutil.rmtree(os.path.expanduser('model/LSTM'), ignore_errors=True)\n",
    "LSTM_training_loop = train_model(LSTMmodel, data_generator, n_steps=10, output_dir='model/LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PWePFGVKnr0t",
    "papermill": {
     "duration": 41.004194,
     "end_time": "2020-10-19T05:28:36.239938",
     "exception": false,
     "start_time": "2020-10-19T05:27:55.235744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "shutil.rmtree(os.path.expanduser('model/SRU'), ignore_errors=True)\n",
    "SRU_training_loop = train_model(SRUmodel, data_generator, n_steps=50_000, output_dir='model/SRU')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Deep N-Gram Models",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 297.094983,
   "end_time": "2020-10-19T05:28:36.576660",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-19T05:23:39.481677",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
