{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT with Transformers/Reformers using Trax.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_WpKodqa9dmJ",
        "VY6_SnLM9dms",
        "WD0ZqedYIpr3",
        "r_8UOdZ_9dnO",
        "v5IDVjXl9dnU",
        "4U_V6nNQ_37u"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmarAlsaqa/trax/blob/master/NMT_with_Transformers_Reformers_using_Trax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAAzPCP8n05S"
      },
      "source": [
        "#@title\n",
        "# Copyright 2021 Google LLC.\n",
        "\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqqdEx7xtHuH"
      },
      "source": [
        "# **NMT with Transformers/Reformers using Trax**\n",
        "\n",
        "A guide to Neural Machine Translation using ***Transformers/Reformers***. Includes a detailed tutorial using ***Trax*** in Google Colaboratory.\n",
        "\n",
        "Machine translation is an important task in natural language processing and could be useful not only for translating one language to another but also for word sense disambiguation. \n",
        "\n",
        "In this Notebook you will:\n",
        "*   Learn how to preprocess your training and evaluation data.\n",
        "*   implement an encoder-decoder system with attention.\n",
        "*   understand how attention works.\n",
        "*   build the NMT model from scratch using Trax.\n",
        "*   learn how to preprocess your training and evaluation data.\n",
        "*   generate translations using greedy and Minimum Bayes Risk (MBR) decoding.\n",
        "\n",
        "This notebook contains a lot of cells taken from [Natural Language Processing Specialization](https://www.coursera.org/specializations/natural-language-processing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8u7YU2uqOXH"
      },
      "source": [
        "# Part (-1): Run on TPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO10zU6I87dc"
      },
      "source": [
        "This notebook was designed to run on TPU.\n",
        "\n",
        "To use TPUs in Colab, click \"Runtime\" on the main menu bar and select Change runtime type. Set \"TPU\" as the hardware accelerator.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QCsYnkLv59s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29c114d1-c940-4411-fcf1-984a34b7f9fa"
      },
      "source": [
        "# Install JAX/TRAX.\n",
        "!pip install --upgrade -q jax\n",
        "!pip install --upgrade -q jaxlib\n",
        "!pip install --upgrade -q trax"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 34.7MB 123kB/s \n",
            "\u001b[K     |████████████████████████████████| 522kB 5.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.4MB 9.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 215kB 52.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 35.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 53.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 52.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 7.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.9MB 56.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 53.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 59.9MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsN3Jxi6vquW",
        "outputId": "ea9f5590-0f13-43f0-dcfe-f075941ca07f"
      },
      "source": [
        "# Make sure the Colab Runtime is set to Accelerator: TPU.\n",
        "import requests\n",
        "import os\n",
        "if 'TPU_DRIVER_MODE' not in globals():\n",
        "  url = 'http://' + os.environ['COLAB_TPU_ADDR'].split(':')[0] + ':8475/requestversion/tpu_driver0.1-dev20191206'\n",
        "  resp = requests.post(url)\n",
        "  TPU_DRIVER_MODE = 1\n",
        "\n",
        "# The following is required to use TPU Driver as JAX's backend.\n",
        "from jax.config import config\n",
        "config.FLAGS.jax_xla_backend = \"tpu_driver\"\n",
        "config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']\n",
        "print(config.FLAGS.jax_backend_target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "grpc://10.43.185.50:8470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVw5457jqlOm"
      },
      "source": [
        "# Part (0): Important Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nA7u_MqG9dmQ",
        "outputId": "741a1e11-319a-4742-e38e-6217da1295e9"
      },
      "source": [
        "import trax\n",
        "from trax.data import inputs\n",
        "from trax import layers as tl\n",
        "from trax.supervised import training\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from termcolor import colored\n",
        "import random\n",
        "\n",
        "!pip list | grep trax"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trax                          1.3.7                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aByNRLKr9dmG"
      },
      "source": [
        "# Part (1):  Data Preparation\n",
        "\n",
        "**You Can jump directly to Trax Data Pipeline (optional) Section and skip 1.1 to 1.5 sections.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WpKodqa9dmJ"
      },
      "source": [
        "## 1.1  Importing the Data\n",
        "We will be using [ParaCrawl](https://paracrawl.eu/), a large multi-lingual translation dataset created by the European Union. All of these datasets are available via [TFDS para_crawl](https://www.tensorflow.org/datasets/catalog/para_crawl). We used English to French dataset. You can try the other avaliable languages by changing the `dataset_name` and `keys`. Or even try another datasets available at TFDS.\n",
        "\n",
        "Notice: It will take a while in the first time to download the dataset. So, it is prefered to specify `data_dir` on Google Drive not in Colab runtime. Try other than para_crawl dataset. since, the para_crawl is a large dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-cIYEHwrhoZ",
        "outputId": "8cb90c0f-db5f-4f07-8e1f-a4e4637d9f33"
      },
      "source": [
        "# MOUNT DRIVE\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEJaYJ5C9dmb"
      },
      "source": [
        "# This will download the train dataset if no data_dir is specified.\n",
        "train_stream_fn = trax.data.TFDS('para_crawl/enfr',\n",
        "                                 data_dir='/content/drive/MyDrive/Colab Notebooks/data/',\n",
        "                                 keys=('en', 'fr'),\n",
        "                                 eval_holdout_size=0.01, # 1% for eval\n",
        "                                 train=True)\n",
        "\n",
        "# Get generator function for the eval set\n",
        "eval_stream_fn = trax.data.TFDS('para_crawl/enfr',\n",
        "                                data_dir='/content/drive/MyDrive/Colab Notebooks/data/',\n",
        "                                keys=('en', 'fr'),\n",
        "                                eval_holdout_size=0.01, # 1% for eval\n",
        "                                train=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk-x0gW9-qsD"
      },
      "source": [
        "You can work with your own datasets instead of loading your dataset with TFDS. Opening a file as shown above creates that generator for you. dont forget to make another function for eval.\n",
        "\n",
        "```python\n",
        "def train_stream_fn():\n",
        "  # provide an infinite generator",
        "  while True:",
        "    # open the first language file (e.g. English sentences)\n",
        "    with open('lang1.csv','r') as f1:\n",
        "      # open the second language file (e.g. French sentences)\n",
        "      with open('lang2.csv','r') as f2:\n",
        "        # looping over the two files to combine the two translation toghether and yields them.\n",
        "        for l1, l2 in zip(f1,f2):\n",
        "          yield (l1, l2)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPlcZf3RLNAg"
      },
      "source": [
        "Notice that TFDS returns a generator *function*.\n",
        "\n",
        "Let's print a a sample pair from our train and eval data. Notice that the raw ouput is represented in bytes (denoted by the `b'` prefix) and these will be converted to strings internally in the next steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16UrIf259dml",
        "outputId": "9a860216-c9fa-4f29-e28a-e9c535feefd4"
      },
      "source": [
        "train_stream = train_stream_fn()\n",
        "print(colored('train data (en, fr) tuple:', 'red'), next(train_stream))\n",
        "print()\n",
        "\n",
        "eval_stream = eval_stream_fn()\n",
        "print(colored('eval data (en, fr) tuple:', 'red'), next(eval_stream))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mtrain data (en, fr) tuple:\u001b[0m (b'Our soldiers are in a poor state; the Germans want grain, they will take it and go back, making it impossible for Soviet power to continue in existence.', b\"Nos soldats ne valent rien ; les Allemands veulent du bl\\xc3\\xa9, ils le prendront et ils battront en retraite apr\\xc3\\xa8s avoir rendu impossible l'existence du pouvoir des Soviets. Dire que la d\\xc3\\xa9mobilisation cesse, c'est se condamner \\xc3\\xa0 \\xc3\\xaatre balay\\xc3\\xa9. Notes\")\n",
            "\n",
            "\u001b[31meval data (en, fr) tuple:\u001b[0m (b'These scrumptious brownies can be part of a healthful eating plan.', b\"Ces succulents brownies peuvent faire partie d'un r\\xc3\\xa9gime alimentaire \\xc3\\xa9quilibr\\xc3\\xa9.\")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWUH9_PNIe5g"
      },
      "source": [
        "Now that we have imported our corpus, we will be preprocessing the sentences into a format that our model can accept. This will be composed of several steps:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY6_SnLM9dms"
      },
      "source": [
        "## 1.2  Tokenization and Formatting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWP3GAoXHiwo"
      },
      "source": [
        "**Tokenizing the sentences using subword representations:** we want to represent each sentence as an array of integers instead of strings. For our application, we will use *subword* representations to tokenize our sentences. This is a common technique to avoid out-of-vocabulary words by allowing parts of words to be represented separately. For example, instead of having separate entries in your vocabulary for \"fear\", \"fearless\", \"fearsome\", \"some\", and \"less\", you can simply store \"fear\", \"some\", and \"less\" then allow your tokenizer to combine these subwords when needed. This allows it to be more flexible so you won't have to save uncommon words explicitly in your vocabulary (e.g. *stylebender*, *nonce*, etc). Tokenizing is done with the `trax.data.Tokenize()` command. The combined subword vocabulary for English, German and French (i.e. `endefr_32k.subword`) is provided by trax. Feel free to open this file to see how the subwords look like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8R2RxvK9dmt"
      },
      "source": [
        "# global variables that state the filename and directory of the vocabulary file\n",
        "VOCAB_FILE = 'endefr_32k.subword'\n",
        "VOCAB_DIR = 'gs://trax-ml/vocabs/'\n",
        "\n",
        "# Tokenize the dataset.\n",
        "tokenized_train_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(train_stream)\n",
        "tokenized_eval_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(eval_stream)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrmCi915HTKA"
      },
      "source": [
        "**Append an end-of-sentence token to each sentence:** We will assign a token (i.e. in this case `1`) to mark the end of a sentence. This will be useful in inference/prediction so we'll know that the model has completed the translation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuolzODV9dm0"
      },
      "source": [
        "# Append EOS at the end of each sentence.\n",
        "\n",
        "# Integer assigned as end-of-sentence (EOS)\n",
        "EOS = 1\n",
        "\n",
        "# generator helper function to append EOS to each sentence\n",
        "def append_eos(stream):\n",
        "    for (inputs, targets) in stream:\n",
        "        inputs_with_eos = list(inputs) + [EOS]\n",
        "        targets_with_eos = list(targets) + [EOS]\n",
        "        yield np.array(inputs_with_eos), np.array(targets_with_eos)\n",
        "\n",
        "# append EOS to the train data\n",
        "tokenized_train_stream = append_eos(tokenized_train_stream)\n",
        "\n",
        "# append EOS to the eval data\n",
        "tokenized_eval_stream = append_eos(tokenized_eval_stream)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbaYhKr99dm8"
      },
      "source": [
        "**Filter long sentences:** We will place a limit on the number of tokens per sentence to ensure we won't run out of memory. This is done with the `trax.data.FilterByLength()` method and you can see its syntax below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Miw7Uu849dm9",
        "outputId": "3a6b35a5-f257-42fb-b914-59534d3f2a76"
      },
      "source": [
        "# Filter too long sentences to not run out of memory.\n",
        "# length_keys=[0, 1] means we filter both English and French sentences, so\n",
        "# both much be not longer that 512 tokens for training / 1024 for eval.\n",
        "filtered_train_stream = trax.data.FilterByLength(\n",
        "    max_length=512, length_keys=[0, 1])(tokenized_train_stream)\n",
        "filtered_eval_stream = trax.data.FilterByLength(\n",
        "    max_length=1024, length_keys=[0, 1])(tokenized_eval_stream)\n",
        "\n",
        "# print a sample input-target pair of tokenized sentences\n",
        "train_input, train_target = next(filtered_train_stream)\n",
        "print(colored(f'Single tokenized example input:', 'red' ), train_input)\n",
        "print(colored(f'Single tokenized example target:', 'red'), train_target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mSingle tokenized example input:\u001b[0m [  107     4  1624  1039     4  6211    34  2544   533  1272 19535   757\n",
            "    15   694  3252   371  8538     3     1]\n",
            "\u001b[31mSingle tokenized example target:\u001b[0m [  812   578    28   485  1791     2 11044    49    18    31  9859     5\n",
            "    10  3965  2994  3077    26   285 12502  5005    49    21  7275 11759\n",
            "     3     1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD0ZqedYIpr3"
      },
      "source": [
        "## 1.3  tokenize & detokenize helper functions\n",
        "\n",
        "- tokenize(): converts a text sentence to its corresponding token list (i.e. list of indices). Also converts words to subwords (parts of words).\n",
        "- detokenize(): converts a token list to its corresponding sentence (i.e. string)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyO5I2e_9dnD"
      },
      "source": [
        "# Setup helper functions for tokenizing and detokenizing sentences\n",
        "def tokenize(input_str, vocab_file=None, vocab_dir=None):\n",
        "    \"\"\"Encodes a string to an array of integers\n",
        "    Args:\n",
        "        input_str (str): human-readable string to encode\n",
        "        vocab_file (str): filename of the vocabulary text file\n",
        "        vocab_dir (str): path to the vocabulary file\n",
        "    Returns:\n",
        "        numpy.ndarray: tokenized version of the input string\n",
        "    \"\"\"\n",
        "    # Set the encoding of the \"end of sentence\" as 1\n",
        "    EOS = 1\n",
        "    # Use the trax.data.tokenize method. It takes streams and returns streams,\n",
        "    # we get around it by making a 1-element stream with `iter`.\n",
        "    inputs =  next(trax.data.tokenize(iter([input_str]),\n",
        "                                      vocab_file=vocab_file, vocab_dir=vocab_dir))\n",
        "    # Mark the end of the sentence with EOS\n",
        "    inputs = list(inputs) + [EOS]\n",
        "    # Adding the batch dimension to the front of the shape\n",
        "    batch_inputs = np.reshape(np.array(inputs), [1, -1])\n",
        "    return batch_inputs\n",
        "\n",
        "def detokenize(integers, vocab_file=None, vocab_dir=None):\n",
        "    \"\"\"Decodes an array of integers to a human readable string\n",
        "    Args:\n",
        "        integers (numpy.ndarray): array of integers to decode\n",
        "        vocab_file (str): filename of the vocabulary text file\n",
        "        vocab_dir (str): path to the vocabulary file \n",
        "    Returns:\n",
        "        str: the decoded sentence.\n",
        "    \"\"\"\n",
        "    # Remove the dimensions of size 1\n",
        "    integers = list(np.squeeze(integers))\n",
        "    # Set the encoding of the \"end of sentence\" as 1\n",
        "    EOS = 1\n",
        "    # Remove the EOS to decode only the original tokens\n",
        "    if EOS in integers:\n",
        "        integers = integers[:integers.index(EOS)]  \n",
        "    return trax.data.detokenize(integers, vocab_file=vocab_file, vocab_dir=vocab_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKfYr4SA9dnH"
      },
      "source": [
        "Let's see how we might use these functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xb7UEVAS9dnI",
        "outputId": "dc1cc233-77ef-4ee2-93cc-34f7ddc586c2"
      },
      "source": [
        "# Detokenize an input-target pair of tokenized sentences\n",
        "print(colored(f'Single detokenized example input:', 'red'), detokenize(train_input, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
        "print(colored(f'Single detokenized example target:', 'red'), detokenize(train_target, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
        "print()\n",
        "\n",
        "# Tokenize and detokenize a word that is not explicitly saved in the vocabulary file.\n",
        "# See how it combines the subwords 'hell' and 'o' to form the word 'hello'.\n",
        "print(colored(f\"tokenize('hello'): \", 'green'), tokenize('hello', vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mSingle detokenized example input:\u001b[0m In the longer term the emphasis on increasing road capacity encourages car-based urban development patterns.\n",
            "\u001b[31mSingle detokenized example target:\u001b[0m Au niveau du long terme, insister sur l’accroissement de la capacité routière encourage le développement urbain basé sur les véhicules personnels.\n",
            "\n",
            "\u001b[32mtokenize('hello'): \u001b[0m [[11068  5505     1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_8UOdZ_9dnO"
      },
      "source": [
        "## 1.4  Bucketing\n",
        "\n",
        "Bucketing the tokenized sentences is an important technique used to speed up training in NLP.\n",
        "Here is a \n",
        "[nice article describing it in detail](https://medium.com/@rashmi.margani/how-to-speed-up-the-training-of-the-sequence-model-using-bucketing-techniques-9e302b0fd976)\n",
        "but the gist is very simple. Our inputs have variable lengths and you want to make these the same when batching groups of sentences together. One way to do that is to pad each sentence to the length of the longest sentence in the dataset. This might lead to some wasted computation though. For example, if there are multiple short sentences with just two tokens, do we want to pad these when the longest sentence is composed of a 100 tokens? Instead of padding with 0s to the maximum length of a sentence each time, we can group our tokenized sentences by length and bucket, as on this image (from the article above):\n",
        "\n",
        "![alt text](https://miro.medium.com/max/700/1*hcGuja_d5Z_rFcgwe9dPow.png)\n",
        "\n",
        "We batch the sentences with similar length together (e.g. the blue sentences in the image above) and only add minimal padding to make them have equal length (usually up to the nearest power of two). This allows to waste less computation when processing padded sequences.\n",
        "In Trax, it is implemented in the [bucket_by_length](https://github.com/google/trax/blob/5fb8aa8c5cb86dabb2338938c745996d5d87d996/trax/supervised/inputs.py#L378) function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUlfg9kX9dnP"
      },
      "source": [
        "# Bucketing to create streams of batches.\n",
        "\n",
        "# Buckets are defined in terms of boundaries and batch sizes.\n",
        "# Batch_sizes[i] determines the batch size for items with length < boundaries[i]\n",
        "# So below, we'll take a batch of 128 sentences of length < 8, 128 if length is\n",
        "# between 8 and 16, and so on. 128 batch is also taken if length is over 256.\n",
        "boundaries =  [  8,  16,  32,  64, 128, 256]\n",
        "batch_sizes = [128, 128, 128, 128, 128, 128, 128]\n",
        "# Notice all is 128. As we are using TPUs, We need the same batch_size to run in parallel.\n",
        "# You can make diffrent batch_sizes if you are using GPU or CPU.\n",
        "\n",
        "# Create the generators.\n",
        "train_batch_stream = trax.data.BucketByLength(\n",
        "    boundaries, batch_sizes,\n",
        "    length_keys=[0, 1]  # As before: count inputs and targets to length.\n",
        ")(filtered_train_stream)\n",
        "\n",
        "eval_batch_stream = trax.data.BucketByLength(\n",
        "    boundaries, batch_sizes,\n",
        "    length_keys=[0, 1]  # As before: count inputs and targets to length.\n",
        ")(filtered_eval_stream)\n",
        "\n",
        "# Add masking for the padding (0s).\n",
        "train_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(train_batch_stream)\n",
        "eval_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(eval_batch_stream)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5IDVjXl9dnU"
      },
      "source": [
        "## 1.5 Exploring the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vX-ukU52No8Q"
      },
      "source": [
        "We will now be displaying some of our data. You will see that the functions defined above (i.e. `tokenize()` and `detokenize()`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zI_Rea2Q9dnV",
        "outputId": "2db581ca-3b3a-450f-9b91-1d924420fa51"
      },
      "source": [
        "input_batch, target_batch, mask_batch = next(train_batch_stream)\n",
        "\n",
        "# let's see the data type of a batch\n",
        "print(\"input_batch data type: \", type(input_batch))\n",
        "print(\"target_batch data type: \", type(target_batch))\n",
        "\n",
        "# let's see the shape of this particular batch (batch length, sentence length)\n",
        "print(\"input_batch shape: \", input_batch.shape)\n",
        "print(\"target_batch shape: \", target_batch.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_batch data type:  <class 'numpy.ndarray'>\n",
            "target_batch data type:  <class 'numpy.ndarray'>\n",
            "input_batch shape:  (128, 64)\n",
            "target_batch shape:  (128, 64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE_ilByVN8zT"
      },
      "source": [
        "The `input_batch` and `target_batch` are Numpy arrays consisting of tokenized English sentences and French sentences respectively. These tokens will later be used to produce embedding vectors for each word in the sentence (so the embedding for a sentence will be a matrix).\n",
        "\n",
        "We can now visually inspect some of the data. You can run the cell below several times to shuffle through the sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vd_71uRi9dnb",
        "outputId": "c25d1a92-8953-4d1b-c916-9bfde6882414"
      },
      "source": [
        "# pick a random index less than the batch size.\n",
        "index = random.randrange(len(input_batch))\n",
        "\n",
        "# use the index to grab an entry from the input and target batch\n",
        "print(colored('THIS IS THE ENGLISH SENTENCE: \\n', 'red'), detokenize(input_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\n",
        "print(colored('THIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \\n ', 'red'), input_batch[index], '\\n')\n",
        "print(colored('THIS IS THE FRENCH TRANSLATION: \\n', 'red'), detokenize(target_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\n",
        "print(colored('THIS IS THE TOKENIZED VERSION OF THE FRENCH TRANSLATION: \\n', 'red'), target_batch[index], '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mTHIS IS THE ENGLISH SENTENCE: \n",
            "\u001b[0m If we do not give the question the same attention here as other alternative propositions, it is because this is an overarching debate that tends to be divisive within social movements and left-wing political parties. It is indeed a major question that would require a lengthy discussion to be dealt with adequately. \n",
            "\n",
            "\u001b[31mTHIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \n",
            " \u001b[0m [  600   100   271    83   993     4   329     4   470   914   859    60\n",
            "   221  3577  1819     2    62    27   382    64    27    50  9052  7500\n",
            "   260   876    29  8480    16    14    53 17380  1965   439   392 16958\n",
            "    11  2220    15  6347   798   563    39   186    27  2780    17   858\n",
            "   329    29   150  3630    17 29834    88  1682    14    53  9780    58\n",
            " 20015     3     1     0] \n",
            "\n",
            "\u001b[31mTHIS IS THE FRENCH TRANSLATION: \n",
            "\u001b[0m Si nous n’y accordons pas ici la même attention qu’aux autres propositions d’alternatives, c’est que le débat traverse et divise tant les mouvements sociaux que les partis de gauche et qu’il est nécessaire d’y consacrer de nombreuses pages pour faire le tour de la question. \n",
            "\n",
            "\u001b[31mTHIS IS THE TOKENIZED VERSION OF THE FRENCH TRANSLATION: \n",
            "\u001b[0m [  983   108    30    31    88 14331   281   102  1257    10   280   914\n",
            "   103    31    89   265  1819    24    31 22892     2   162    31    37\n",
            "    36    26  1124 28501    12 17380    32   854    21 16959  4369    36\n",
            "    21 10943    16     5 19387    12   103    31    72    37  1101    24\n",
            "    31    88 17458     5  1397  4763    40   287    26  3619     5    10\n",
            "   329     3     1     0] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDSPHBZaeRAW"
      },
      "source": [
        "## Trax Data Pipeline (optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP2RACXYeTse"
      },
      "source": [
        "Those were the steps needed to prepare the data (steps from 1.1 to 1.5) But you could simply use [Trax data pipeline](https://trax-ml.readthedocs.io/en/latest/notebooks/trax_intro.html#Data) `trax.data.Serial` in the next cell. **if you run this cell you should skip (steps from 1.1 to 1.5).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUPhstH70Xzu"
      },
      "source": [
        "You can work with your own datasets instead of loading your dataset with TFDS you can simply replace the TFDS call with an `lambda _: train_stream_fn()`\n",
        "Everything in tf.Serial is a generator. Opening a file as shown above creates that generator for you.\n",
        "\n",
        "```python\n",
        "def train_stream_fn():\n",
        "  # open the first language file (e.g. English sentences)\n",
        "  with open('lang1.csv','r') as f1:\n",
        "    # open the second language file (e.g. French sentences)\n",
        "    with open('lang2.csv','r') as f2:\n",
        "      # looping over the two files to combine the two translation toghether and yields them.\n",
        "      for l1, l2 in zip(f1,f2):\n",
        "        yield (l1, l2)\n",
        "```\n",
        "\n",
        "and then add\n",
        "```python\n",
        "lambda _: train_stream_fn()\n",
        "```\n",
        "to `trax.data.Serial()` instead of \n",
        "```python\n",
        "trax.data.TFDS('para_crawl/enfr',\n",
        "               data_dir='/content/drive/MyDrive/Colab Notebooks/data/',\n",
        "               keys=('en', 'fr'),\n",
        "               eval_holdout_size=0.01, # 1% for eval\n",
        "               train=True)\n",
        "```\n",
        "for both the training and eval streams."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwwICFzpCTMa",
        "outputId": "ed0b59ac-88d8-423f-b958-c683b6964f37"
      },
      "source": [
        "# MOUNT DRIVE\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52LAeuAETG9o"
      },
      "source": [
        "# if you run this cell you should skip (steps from 1.1 to 1.5).\n",
        "\n",
        "# global variables that state the filename and directory of the vocabulary file\n",
        "VOCAB_FILE = 'endefr_32k.subword'\n",
        "VOCAB_DIR = 'gs://trax-ml/vocabs/'\n",
        "\n",
        "EOS = 1\n",
        "\n",
        "# generator helper function to append EOS to each sentence\n",
        "def append_eos(stream):\n",
        "    for (inputs, targets) in stream:\n",
        "        inputs_with_eos = list(inputs) + [EOS]\n",
        "        targets_with_eos = list(targets) + [EOS]\n",
        "        yield np.array(inputs_with_eos), np.array(targets_with_eos)\n",
        "\n",
        "train_batches_stream = trax.data.Serial(\n",
        "    trax.data.TFDS('para_crawl/enfr',\n",
        "                   data_dir='/content/drive/MyDrive/Colab Notebooks/data/',\n",
        "                   keys=('en', 'fr'),\n",
        "                   eval_holdout_size=0.01, # 1% for eval\n",
        "                   train=True), # replace TFDS with lambda _: train_stream_fn() if you want to run with your own data\n",
        "    trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR),\n",
        "    lambda _: append_eos(_),\n",
        "    trax.data.Shuffle(),\n",
        "    trax.data.FilterByLength(max_length=512, length_keys=[0, 1]),\n",
        "    trax.data.BucketByLength(boundaries =  [  8,  16,  32,  64, 128, 256],\n",
        "                             batch_sizes = [128, 128, 128, 128, 128, 128, 128],\n",
        "                             length_keys=[0, 1]),\n",
        "    trax.data.AddLossWeights(id_to_mask=0)\n",
        "  )\n",
        "\n",
        "eval_batches_stream = trax.data.Serial(\n",
        "    trax.data.TFDS('para_crawl/enfr',\n",
        "                   data_dir='/content/drive/MyDrive/Colab Notebooks/data/',\n",
        "                   keys=('en', 'fr'),\n",
        "                   eval_holdout_size=0.01, # 1% for eval\n",
        "                   train=False),\n",
        "    trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR),\n",
        "    lambda _: append_eos(_),\n",
        "    trax.data.Shuffle(),\n",
        "    trax.data.FilterByLength(max_length=1024, length_keys=[0, 1]),\n",
        "    trax.data.BucketByLength(boundaries =  [  8,  16,  32,  64, 128, 256],\n",
        "                             batch_sizes = [128, 128, 128, 128, 128, 128, 128],\n",
        "                             length_keys=[0, 1]),\n",
        "    trax.data.AddLossWeights(id_to_mask=0)\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWvu5PraqBQx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34480b60-6a40-4be6-f51c-4e481c2e1bc3"
      },
      "source": [
        "# Exploring the data\n",
        "train_batch_stream = train_batches_stream()\n",
        "eval_batch_stream = eval_batches_stream()\n",
        "input_batch, target_batch, mask_batch = next(train_batch_stream)\n",
        "# let's see the data type of a batch\n",
        "print(\"input_batch data type: \", type(input_batch))\n",
        "print(\"target_batch data type: \", type(target_batch))\n",
        "# let's see the shape of this particular batch (batch length, sentence length)\n",
        "print(\"input_batch shape: \", input_batch.shape)\n",
        "print(\"target_batch shape: \", target_batch.shape)\n",
        "\n",
        "# pick a random index less than the batch size.\n",
        "index = random.randrange(len(input_batch))\n",
        "# use the index to grab an entry from the input and target batch\n",
        "print(colored('ENGLISH SENTENCE: \\n', 'red'), trax.data.detokenize(input_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\n",
        "print(colored('THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \\n ', 'red'), input_batch[index], '\\n')\n",
        "print(colored('THE FRENCH TRANSLATION: \\n', 'red'), trax.data.detokenize(target_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\n",
        "print(colored('THE TOKENIZED VERSION OF THE FRENCH TRANSLATION: \\n', 'red'), target_batch[index], '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_batch data type:  <class 'numpy.ndarray'>\n",
            "target_batch data type:  <class 'numpy.ndarray'>\n",
            "input_batch shape:  (128, 32)\n",
            "target_batch shape:  (128, 32)\n",
            "\u001b[31mENGLISH SENTENCE: \n",
            "\u001b[0m Instead they can submit section 2 of the application to Health Canada and have it added to their original application. \n",
            "\n",
            "\u001b[31mTHE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \n",
            " \u001b[0m [13284   182   136  9547  1098   112     8     4   487    14  6051   141\n",
            "    11    82    62  4183    14   148  1957   487     3     1     0     0\n",
            "     0     0     0     0     0     0     0     0] \n",
            "\n",
            "\u001b[31mTHE FRENCH TRANSLATION: \n",
            "\u001b[0m Ils peuvent plutôt soumettre la section 2 du formulaire à Santé Canada pour qu'elle soit ajoutée à leur demande originale. \n",
            "\n",
            "\u001b[31mTHE TOKENIZED VERSION OF THE FRENCH TRANSLATION: \n",
            "\u001b[0m [ 2621   589  2726 13657    10  1098   112    28 19418     6    23  7163\n",
            "   141    40   103     7   252   419 11275    23   267   775  4948     3\n",
            "     1     0     0     0     0     0     0     0] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-69Mr-9_VEk"
      },
      "source": [
        "# Part (2):  Model\n",
        "\n",
        "Now that we’ve seen preprocessing, it’s time to move into Modeling itself. Trax allows the use of Predefined Models, such as:\n",
        " - Seq2Seq with Attention\n",
        " - BERT\n",
        " - Transformer\n",
        " - Reformer\n",
        "\n",
        "We will be using Transformer in this Notebook As Trax provided a pretrained Transformer NMT Model which is traind on English to German dataset and We now are going to train it on English to French dataset and get a very close results to the one provide by Google Brain Team.\n",
        "\n",
        "You can simply change `trax.models.Transformer` in the next cell to `trax.models.Reformer` to use the Reformer model.\n",
        "\n",
        "```python\n",
        "# you could check the available pretrained models and vocab files provided by trax by running:\n",
        "!gsutil ls gs://trax-ml/\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkAuvdOErAlP"
      },
      "source": [
        "# Create a Transformer model.\n",
        "model = trax.models.Transformer(\n",
        "    input_vocab_size=33600,\n",
        "    d_model=512, d_ff=2048, dropout = 0.1,\n",
        "    n_heads=8, n_encoder_layers=6, n_decoder_layers=6,\n",
        "    max_len=2048, mode='train')\n",
        "\n",
        "# Pre-trained Transformer model config in gs://trax-ml/models/translation/ende_wmt32k.gin\n",
        "# Initialize Transformer using pre-trained weights.\n",
        "model.init_from_file('gs://trax-ml/models/translation/ende_wmt32k.pkl.gz',\n",
        "                     weights_only=True)\n",
        "\n",
        "# You also, could intiate the model from an output checpoint.\n",
        "# simply change 'gs://trax-ml/models/translation/ende_wmt32k.pkl.gz' to 'output_dir/ + last_checkpoint'\n",
        "# for example:\n",
        "# model.init_from_file('/content/drive/MyDrive/Colab Notebooks/Transformer_FR_pretrained_336/model.pkl.gz',\n",
        "#                      weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2p0AGzlKQusn"
      },
      "source": [
        "You could have a peek at the model layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvmdtOfeZ9Ff"
      },
      "source": [
        "# model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8FfOMp59doX"
      },
      "source": [
        "# Part (3):  Training\n",
        "We will now be training our model in this section. Doing supervised training in Trax is pretty straightforward (short example [here](https://trax-ml.readthedocs.io/en/latest/notebooks/trax_intro.html#Supervised-training)). We will be instantiating three classes for this: `TrainTask`, `EvalTask`, and `Loop`. Let's take a closer look at each of these in the sections below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re1ZHUac9doY"
      },
      "source": [
        "## 3.1  TrainTask\n",
        "\n",
        "The [TrainTask](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask) class allows us to define the labeled data to use for training and the feedback mechanisms to compute the loss and update the weights. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFP83q7S9doZ"
      },
      "source": [
        "train_task = training.TrainTask(\n",
        "    # use the train batch stream as labeled data\n",
        "    labeled_data= train_batch_stream,\n",
        "    # use the cross entropy loss with LogSoftmax\n",
        "    loss_layer= tl.CrossEntropyLossWithLogSoftmax(),\n",
        "    # use the Adafactor optimizer with learning rate of 0.001\n",
        "    optimizer= trax.optimizers.Adafactor(learning_rate=0.001, epsilon1=1e-30),\n",
        "    # have 500 warmup steps\n",
        "    lr_schedule= trax.lr.multifactor(constant=1.0, warmup_steps=500),\n",
        "    # have a checkpoint every 100 steps\n",
        "    n_steps_per_checkpoint= 100,\n",
        "    # saving a checkpoint every 1000 steps on the output_dir\n",
        "    n_steps_per_permanent_checkpoint = 1000\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EQI-c999doi"
      },
      "source": [
        "## 3.2  EvalTask\n",
        "\n",
        "The [EvalTask](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask) on the other hand allows us to see how the model is doing while training. For our application, we want it to report the cross entropy loss with LogSoftmax and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5hVQ0Qd9doj"
      },
      "source": [
        "eval_task = training.EvalTask(\n",
        "    # use the eval batch stream as labeled data\n",
        "    labeled_data=eval_batch_stream,\n",
        "    # use the cross entropy loss with LogSoftmax and accuracy as metrics\n",
        "    metrics=[tl.CrossEntropyLossWithLogSoftmax(), tl.WeightedCategoryAccuracy()],\n",
        "    # you could specify the number of eval batch by n_eval_batches = 64 or any other number\n",
        "    # but it not specified here as we want to evaluate the whole eval data\n",
        "    # n_eval_batches = 64\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14pSLHEw9dol"
      },
      "source": [
        "## 3.3  Loop\n",
        "\n",
        "The [Loop](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop) class defines the model we will train as well as the train and eval tasks to execute. Its `run()` method allows us to execute the training for a specified number of steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdnRbEAz9dom"
      },
      "source": [
        "# define the output directory\n",
        "output_dir = '/content/drive/MyDrive/Colab Notebooks/Transformer_FR_pretrained_336'\n",
        "\n",
        "# # remove old model if it exists. restarts training.\n",
        "# !rm -rf output_dir\n",
        "\n",
        "# define the training loop\n",
        "training_loop = training.Loop(model,\n",
        "                              train_task,\n",
        "                              eval_tasks=[eval_task],\n",
        "                              output_dir=output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRk-1Wsu9doo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1459c595-b218-4be8-d6ea-805147ca20c5"
      },
      "source": [
        "# Start Training!\n",
        "training_loop.run(5000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step      1: Total number of trainable weights: 80370196\n",
            "Step      1: Ran 1 train steps in 130.79 secs\n",
            "Step      1: train CrossEntropyLossWithLogSoftmax |  9.85205269\n",
            "Step      1: eval  CrossEntropyLossWithLogSoftmax |  9.71523285\n",
            "Step      1: eval        WeightedCategoryAccuracy |  0.08454708\n",
            "\n",
            "Step    100: Ran 99 train steps in 487.01 secs\n",
            "Step    100: train CrossEntropyLossWithLogSoftmax |  7.02561331\n",
            "Step    100: eval  CrossEntropyLossWithLogSoftmax |  5.81694698\n",
            "Step    100: eval        WeightedCategoryAccuracy |  0.22895759\n",
            "\n",
            "Step    200: Ran 100 train steps in 177.36 secs\n",
            "Step    200: train CrossEntropyLossWithLogSoftmax |  5.24865103\n",
            "Step    200: eval  CrossEntropyLossWithLogSoftmax |  4.46555328\n",
            "Step    200: eval        WeightedCategoryAccuracy |  0.34939855\n",
            "\n",
            "Step    300: Ran 100 train steps in 70.54 secs\n",
            "Step    300: train CrossEntropyLossWithLogSoftmax |  4.33149672\n",
            "Step    300: eval  CrossEntropyLossWithLogSoftmax |  3.83377051\n",
            "Step    300: eval        WeightedCategoryAccuracy |  0.42087770\n",
            "\n",
            "Step    400: Ran 100 train steps in 67.50 secs\n",
            "Step    400: train CrossEntropyLossWithLogSoftmax |  3.96885633\n",
            "Step    400: eval  CrossEntropyLossWithLogSoftmax |  4.21221638\n",
            "Step    400: eval        WeightedCategoryAccuracy |  0.34850734\n",
            "\n",
            "Step    500: Ran 100 train steps in 71.36 secs\n",
            "Step    500: train CrossEntropyLossWithLogSoftmax |  3.83042574\n",
            "Step    500: eval  CrossEntropyLossWithLogSoftmax |  3.64122152\n",
            "Step    500: eval        WeightedCategoryAccuracy |  0.41977778\n",
            "\n",
            "Step    600: Ran 100 train steps in 76.19 secs\n",
            "Step    600: train CrossEntropyLossWithLogSoftmax |  3.35245323\n",
            "Step    600: eval  CrossEntropyLossWithLogSoftmax |  3.23352051\n",
            "Step    600: eval        WeightedCategoryAccuracy |  0.47931033\n",
            "\n",
            "Step    700: Ran 100 train steps in 79.46 secs\n",
            "Step    700: train CrossEntropyLossWithLogSoftmax |  3.10961103\n",
            "Step    700: eval  CrossEntropyLossWithLogSoftmax |  3.15166879\n",
            "Step    700: eval        WeightedCategoryAccuracy |  0.46864897\n",
            "\n",
            "Step    800: Ran 100 train steps in 81.76 secs\n",
            "Step    800: train CrossEntropyLossWithLogSoftmax |  2.93149567\n",
            "Step    800: eval  CrossEntropyLossWithLogSoftmax |  3.35616207\n",
            "Step    800: eval        WeightedCategoryAccuracy |  0.44215840\n",
            "\n",
            "Step    900: Ran 100 train steps in 79.12 secs\n",
            "Step    900: train CrossEntropyLossWithLogSoftmax |  2.82020950\n",
            "Step    900: eval  CrossEntropyLossWithLogSoftmax |  2.36338472\n",
            "Step    900: eval        WeightedCategoryAccuracy |  0.57924318\n",
            "\n",
            "Step   1000: Ran 100 train steps in 104.90 secs\n",
            "Step   1000: train CrossEntropyLossWithLogSoftmax |  2.68358994\n",
            "Step   1000: eval  CrossEntropyLossWithLogSoftmax |  2.56412911\n",
            "Step   1000: eval        WeightedCategoryAccuracy |  0.54872346\n",
            "\n",
            "Step   1100: Ran 100 train steps in 83.85 secs\n",
            "Step   1100: train CrossEntropyLossWithLogSoftmax |  2.58823633\n",
            "Step   1100: eval  CrossEntropyLossWithLogSoftmax |  2.62969518\n",
            "Step   1100: eval        WeightedCategoryAccuracy |  0.52579981\n",
            "\n",
            "Step   1200: Ran 100 train steps in 88.05 secs\n",
            "Step   1200: train CrossEntropyLossWithLogSoftmax |  2.51080465\n",
            "Step   1200: eval  CrossEntropyLossWithLogSoftmax |  2.52758622\n",
            "Step   1200: eval        WeightedCategoryAccuracy |  0.53838688\n",
            "\n",
            "Step   1300: Ran 100 train steps in 90.28 secs\n",
            "Step   1300: train CrossEntropyLossWithLogSoftmax |  2.46406817\n",
            "Step   1300: eval  CrossEntropyLossWithLogSoftmax |  2.31228042\n",
            "Step   1300: eval        WeightedCategoryAccuracy |  0.57349908\n",
            "\n",
            "Step   1400: Ran 100 train steps in 90.89 secs\n",
            "Step   1400: train CrossEntropyLossWithLogSoftmax |  2.39312744\n",
            "Step   1400: eval  CrossEntropyLossWithLogSoftmax |  2.07446051\n",
            "Step   1400: eval        WeightedCategoryAccuracy |  0.63776493\n",
            "\n",
            "Step   1500: Ran 100 train steps in 94.41 secs\n",
            "Step   1500: train CrossEntropyLossWithLogSoftmax |  2.35005140\n",
            "Step   1500: eval  CrossEntropyLossWithLogSoftmax |  2.32324076\n",
            "Step   1500: eval        WeightedCategoryAccuracy |  0.57490349\n",
            "\n",
            "Step   1600: Ran 100 train steps in 93.66 secs\n",
            "Step   1600: train CrossEntropyLossWithLogSoftmax |  2.31463027\n",
            "Step   1600: eval  CrossEntropyLossWithLogSoftmax |  2.30394077\n",
            "Step   1600: eval        WeightedCategoryAccuracy |  0.57527685\n",
            "\n",
            "Step   1700: Ran 100 train steps in 94.34 secs\n",
            "Step   1700: train CrossEntropyLossWithLogSoftmax |  2.23612332\n",
            "Step   1700: eval  CrossEntropyLossWithLogSoftmax |  2.14128780\n",
            "Step   1700: eval        WeightedCategoryAccuracy |  0.60737085\n",
            "\n",
            "Step   1800: Ran 100 train steps in 91.89 secs\n",
            "Step   1800: train CrossEntropyLossWithLogSoftmax |  2.23225784\n",
            "Step   1800: eval  CrossEntropyLossWithLogSoftmax |  2.52646112\n",
            "Step   1800: eval        WeightedCategoryAccuracy |  0.52996337\n",
            "\n",
            "Step   1900: Ran 100 train steps in 92.47 secs\n",
            "Step   1900: train CrossEntropyLossWithLogSoftmax |  2.17324281\n",
            "Step   1900: eval  CrossEntropyLossWithLogSoftmax |  2.50346041\n",
            "Step   1900: eval        WeightedCategoryAccuracy |  0.53843790\n",
            "\n",
            "Step   2000: Ran 100 train steps in 113.97 secs\n",
            "Step   2000: train CrossEntropyLossWithLogSoftmax |  2.14945030\n",
            "Step   2000: eval  CrossEntropyLossWithLogSoftmax |  2.17006946\n",
            "Step   2000: eval        WeightedCategoryAccuracy |  0.59461838\n",
            "\n",
            "Step   2100: Ran 100 train steps in 95.56 secs\n",
            "Step   2100: train CrossEntropyLossWithLogSoftmax |  2.13827372\n",
            "Step   2100: eval  CrossEntropyLossWithLogSoftmax |  2.08724308\n",
            "Step   2100: eval        WeightedCategoryAccuracy |  0.60836774\n",
            "\n",
            "Step   2200: Ran 100 train steps in 95.67 secs\n",
            "Step   2200: train CrossEntropyLossWithLogSoftmax |  2.10173893\n",
            "Step   2200: eval  CrossEntropyLossWithLogSoftmax |  1.84778070\n",
            "Step   2200: eval        WeightedCategoryAccuracy |  0.66895270\n",
            "\n",
            "Step   2300: Ran 100 train steps in 99.50 secs\n",
            "Step   2300: train CrossEntropyLossWithLogSoftmax |  2.07918763\n",
            "Step   2300: eval  CrossEntropyLossWithLogSoftmax |  2.18345213\n",
            "Step   2300: eval        WeightedCategoryAccuracy |  0.58705992\n",
            "\n",
            "Step   2400: Ran 100 train steps in 100.64 secs\n",
            "Step   2400: train CrossEntropyLossWithLogSoftmax |  2.06117034\n",
            "Step   2400: eval  CrossEntropyLossWithLogSoftmax |  2.10406661\n",
            "Step   2400: eval        WeightedCategoryAccuracy |  0.61676151\n",
            "\n",
            "Step   2500: Ran 100 train steps in 99.83 secs\n",
            "Step   2500: train CrossEntropyLossWithLogSoftmax |  2.02957368\n",
            "Step   2500: eval  CrossEntropyLossWithLogSoftmax |  2.14476347\n",
            "Step   2500: eval        WeightedCategoryAccuracy |  0.58916318\n",
            "\n",
            "Step   2600: Ran 100 train steps in 99.18 secs\n",
            "Step   2600: train CrossEntropyLossWithLogSoftmax |  2.01416183\n",
            "Step   2600: eval  CrossEntropyLossWithLogSoftmax |  1.93275166\n",
            "Step   2600: eval        WeightedCategoryAccuracy |  0.64312029\n",
            "\n",
            "Step   2700: Ran 100 train steps in 105.54 secs\n",
            "Step   2700: train CrossEntropyLossWithLogSoftmax |  1.98193300\n",
            "Step   2700: eval  CrossEntropyLossWithLogSoftmax |  1.70399988\n",
            "Step   2700: eval        WeightedCategoryAccuracy |  0.67673290\n",
            "\n",
            "Step   2800: Ran 100 train steps in 103.56 secs\n",
            "Step   2800: train CrossEntropyLossWithLogSoftmax |  1.98967147\n",
            "Step   2800: eval  CrossEntropyLossWithLogSoftmax |  2.26600218\n",
            "Step   2800: eval        WeightedCategoryAccuracy |  0.57345927\n",
            "\n",
            "Step   2900: Ran 100 train steps in 106.24 secs\n",
            "Step   2900: train CrossEntropyLossWithLogSoftmax |  1.96915519\n",
            "Step   2900: eval  CrossEntropyLossWithLogSoftmax |  1.83608222\n",
            "Step   2900: eval        WeightedCategoryAccuracy |  0.64486253\n",
            "\n",
            "Step   3000: Ran 100 train steps in 123.97 secs\n",
            "Step   3000: train CrossEntropyLossWithLogSoftmax |  1.95634198\n",
            "Step   3000: eval  CrossEntropyLossWithLogSoftmax |  1.96534336\n",
            "Step   3000: eval        WeightedCategoryAccuracy |  0.61572355\n",
            "\n",
            "Step   3100: Ran 100 train steps in 101.74 secs\n",
            "Step   3100: train CrossEntropyLossWithLogSoftmax |  1.93480480\n",
            "Step   3100: eval  CrossEntropyLossWithLogSoftmax |  2.07498431\n",
            "Step   3100: eval        WeightedCategoryAccuracy |  0.58850276\n",
            "\n",
            "Step   3200: Ran 100 train steps in 106.99 secs\n",
            "Step   3200: train CrossEntropyLossWithLogSoftmax |  1.92409098\n",
            "Step   3200: eval  CrossEntropyLossWithLogSoftmax |  1.75855708\n",
            "Step   3200: eval        WeightedCategoryAccuracy |  0.65730476\n",
            "\n",
            "Step   3300: Ran 100 train steps in 104.48 secs\n",
            "Step   3300: train CrossEntropyLossWithLogSoftmax |  1.89491403\n",
            "Step   3300: eval  CrossEntropyLossWithLogSoftmax |  1.59045112\n",
            "Step   3300: eval        WeightedCategoryAccuracy |  0.68677223\n",
            "\n",
            "Step   3400: Ran 100 train steps in 106.04 secs\n",
            "Step   3400: train CrossEntropyLossWithLogSoftmax |  1.89865410\n",
            "Step   3400: eval  CrossEntropyLossWithLogSoftmax |  1.97923064\n",
            "Step   3400: eval        WeightedCategoryAccuracy |  0.61284077\n",
            "\n",
            "Step   3500: Ran 100 train steps in 103.33 secs\n",
            "Step   3500: train CrossEntropyLossWithLogSoftmax |  1.88124871\n",
            "Step   3500: eval  CrossEntropyLossWithLogSoftmax |  1.87313676\n",
            "Step   3500: eval        WeightedCategoryAccuracy |  0.64622855\n",
            "\n",
            "Step   3600: Ran 100 train steps in 103.66 secs\n",
            "Step   3600: train CrossEntropyLossWithLogSoftmax |  1.86413860\n",
            "Step   3600: eval  CrossEntropyLossWithLogSoftmax |  1.93156767\n",
            "Step   3600: eval        WeightedCategoryAccuracy |  0.61913443\n",
            "\n",
            "Step   3700: Ran 100 train steps in 104.92 secs\n",
            "Step   3700: train CrossEntropyLossWithLogSoftmax |  1.85850441\n",
            "Step   3700: eval  CrossEntropyLossWithLogSoftmax |  1.82424903\n",
            "Step   3700: eval        WeightedCategoryAccuracy |  0.64653146\n",
            "\n",
            "Step   3800: Ran 100 train steps in 108.30 secs\n",
            "Step   3800: train CrossEntropyLossWithLogSoftmax |  1.84594476\n",
            "Step   3800: eval  CrossEntropyLossWithLogSoftmax |  1.89609993\n",
            "Step   3800: eval        WeightedCategoryAccuracy |  0.63271540\n",
            "\n",
            "Step   3900: Ran 100 train steps in 106.10 secs\n",
            "Step   3900: train CrossEntropyLossWithLogSoftmax |  1.81989634\n",
            "Step   3900: eval  CrossEntropyLossWithLogSoftmax |  2.22522569\n",
            "Step   3900: eval        WeightedCategoryAccuracy |  0.57733458\n",
            "\n",
            "Step   4000: Ran 100 train steps in 122.86 secs\n",
            "Step   4000: train CrossEntropyLossWithLogSoftmax |  1.82069206\n",
            "Step   4000: eval  CrossEntropyLossWithLogSoftmax |  1.69654596\n",
            "Step   4000: eval        WeightedCategoryAccuracy |  0.66414380\n",
            "\n",
            "Step   4100: Ran 100 train steps in 105.58 secs\n",
            "Step   4100: train CrossEntropyLossWithLogSoftmax |  1.81504095\n",
            "Step   4100: eval  CrossEntropyLossWithLogSoftmax |  1.82490277\n",
            "Step   4100: eval        WeightedCategoryAccuracy |  0.64850724\n",
            "\n",
            "Step   4200: Ran 100 train steps in 103.48 secs\n",
            "Step   4200: train CrossEntropyLossWithLogSoftmax |  1.79890764\n",
            "Step   4200: eval  CrossEntropyLossWithLogSoftmax |  1.94082224\n",
            "Step   4200: eval        WeightedCategoryAccuracy |  0.61675012\n",
            "\n",
            "Step   4300: Ran 100 train steps in 105.42 secs\n",
            "Step   4300: train CrossEntropyLossWithLogSoftmax |  1.78839958\n",
            "Step   4300: eval  CrossEntropyLossWithLogSoftmax |  1.64301860\n",
            "Step   4300: eval        WeightedCategoryAccuracy |  0.68001771\n",
            "\n",
            "Step   4400: Ran 100 train steps in 107.00 secs\n",
            "Step   4400: train CrossEntropyLossWithLogSoftmax |  1.77166772\n",
            "Step   4400: eval  CrossEntropyLossWithLogSoftmax |  1.98841286\n",
            "Step   4400: eval        WeightedCategoryAccuracy |  0.61240149\n",
            "\n",
            "Step   4500: Ran 100 train steps in 107.03 secs\n",
            "Step   4500: train CrossEntropyLossWithLogSoftmax |  1.77967501\n",
            "Step   4500: eval  CrossEntropyLossWithLogSoftmax |  1.55927932\n",
            "Step   4500: eval        WeightedCategoryAccuracy |  0.69401485\n",
            "\n",
            "Step   4600: Ran 100 train steps in 104.64 secs\n",
            "Step   4600: train CrossEntropyLossWithLogSoftmax |  1.77094245\n",
            "Step   4600: eval  CrossEntropyLossWithLogSoftmax |  1.78488588\n",
            "Step   4600: eval        WeightedCategoryAccuracy |  0.64639080\n",
            "\n",
            "Step   4700: Ran 100 train steps in 107.92 secs\n",
            "Step   4700: train CrossEntropyLossWithLogSoftmax |  1.77308905\n",
            "Step   4700: eval  CrossEntropyLossWithLogSoftmax |  1.85960603\n",
            "Step   4700: eval        WeightedCategoryAccuracy |  0.63444734\n",
            "\n",
            "Step   4800: Ran 100 train steps in 103.30 secs\n",
            "Step   4800: train CrossEntropyLossWithLogSoftmax |  1.76320994\n",
            "Step   4800: eval  CrossEntropyLossWithLogSoftmax |  1.62576365\n",
            "Step   4800: eval        WeightedCategoryAccuracy |  0.67749906\n",
            "\n",
            "Step   4900: Ran 100 train steps in 104.57 secs\n",
            "Step   4900: train CrossEntropyLossWithLogSoftmax |  1.75034785\n",
            "Step   4900: eval  CrossEntropyLossWithLogSoftmax |  2.15475702\n",
            "Step   4900: eval        WeightedCategoryAccuracy |  0.59079874\n",
            "\n",
            "Step   5000: Ran 100 train steps in 132.27 secs\n",
            "Step   5000: train CrossEntropyLossWithLogSoftmax |  1.74079084\n",
            "Step   5000: eval  CrossEntropyLossWithLogSoftmax |  1.76899207\n",
            "Step   5000: eval        WeightedCategoryAccuracy |  0.64984393\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SftZvkIl_4ko"
      },
      "source": [
        "## More Steps (optional)\n",
        "\n",
        "As we have specified the `n_steps_per_permanent_checkpoint` in `training.TrainTask` it saves checkpoint in `output_dir` after the specified number of steps. So, if you have face runtime disconnection or you want to train the model for more number of steps to improve the result, you could load last checkpoint saved and load it using `training_loop.load_checkpoint`. \n",
        "\n",
        "This is an optional way. you could have used `model.init_from_file` as in (Part (2): Model) cells. change 'gs://trax-ml/models/translation/ende_wmt32k.pkl.gz' to 'output_dir/ + last_checkpoint'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBq6EZy6_4Lo"
      },
      "source": [
        "output_dir = '/content/drive/MyDrive/Colab Notebooks/Transformer_FR_pretrained_336/'\n",
        "\n",
        "# This loads a checkpoint:\n",
        "training_loop.load_checkpoint(directory=output_dir, filename=\"model.pkl.gz\")\n",
        "# Continue training:\n",
        "training_loop.run(5000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JHimNjsmHUR"
      },
      "source": [
        "## Tensorboard (optional)\n",
        "The Trax training loop optimizes training, creates TensorBoard logs and model checkpoints for you. you could simply visualize them using the following:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfCy3oZAuron"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdcy6NP1uxOI"
      },
      "source": [
        "%tensorboard --logdir output_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIy5wc90m0ZW"
      },
      "source": [
        "if it is not loading properly, and for example your `output_dir` is:\n",
        "\n",
        "```python\n",
        "output_dir = '/content/drive/MyDrive/Colab Notebooks/Transformer_FR_pretrained_336'\n",
        "```\n",
        "add:\n",
        "```\n",
        "%cd '/content/drive/MyDrive/Colab Notebooks/'\n",
        "```\n",
        "before:\n",
        "```\n",
        "%tensorboard --logdir output_dir\n",
        "```\n",
        "and change it to:\n",
        "```\n",
        "%tensorboard --logdir Transformer_FR_pretrained_336\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WXTnjBJ9dov"
      },
      "source": [
        "# Part (4):  Testing\n",
        "\n",
        "We will now be using the model you just trained to translate English sentences to French. We will implement this with two functions: The first allows you to identify the next symbol (i.e. output token). The second one takes care of combining the entire translated string.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g9O_h-R9do0"
      },
      "source": [
        "## 4.1  Decoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH-imC6U-jBn"
      },
      "source": [
        "# Setup helper functions for tokenizing and detokenizing sentences\n",
        "def tokenize(input_str, vocab_file=None, vocab_dir=None):\n",
        "    \"\"\"Encodes a string to an array of integers\n",
        "    Args:\n",
        "        input_str (str): human-readable string to encode\n",
        "        vocab_file (str): filename of the vocabulary text file\n",
        "        vocab_dir (str): path to the vocabulary file\n",
        "    Returns:\n",
        "        numpy.ndarray: tokenized version of the input string\n",
        "    \"\"\"\n",
        "    # Set the encoding of the \"end of sentence\" as 1\n",
        "    EOS = 1\n",
        "    # Use the trax.data.tokenize method. It takes streams and returns streams,\n",
        "    # we get around it by making a 1-element stream with `iter`.\n",
        "    inputs = next(trax.data.tokenize(iter([input_str]),\n",
        "                                      vocab_file=vocab_file, vocab_dir=vocab_dir))\n",
        "    # Mark the end of the sentence with EOS\n",
        "    inputs = list(inputs) + [EOS]\n",
        "    # Adding the batch dimension to the front of the shape\n",
        "    batch_inputs = np.reshape(np.array(inputs), [1, -1])\n",
        "    return batch_inputs\n",
        "\n",
        "def detokenize(integers, vocab_file=None, vocab_dir=None):\n",
        "    \"\"\"Decodes an array of integers to a human readable string\n",
        "    Args:\n",
        "        integers (numpy.ndarray): array of integers to decode\n",
        "        vocab_file (str): filename of the vocabulary text file\n",
        "        vocab_dir (str): path to the vocabulary file \n",
        "    Returns:\n",
        "        str: the decoded sentence.\n",
        "    \"\"\"\n",
        "    # Remove the dimensions of size 1\n",
        "    integers = list(np.squeeze(integers))\n",
        "    # Set the encoding of the \"end of sentence\" as 1\n",
        "    EOS = 1\n",
        "    # Remove the EOS to decode only the original tokens\n",
        "    if EOS in integers:\n",
        "        integers = integers[:integers.index(EOS)]  \n",
        "    return trax.data.detokenize(integers, vocab_file=vocab_file, vocab_dir=vocab_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3ud8xnDGL-5"
      },
      "source": [
        "There are several ways to get the next token when translating a sentence. For instance, we can just get the most probable token at each step (i.e. greedy decoding) or get a sample from a distribution. We can generalize the implementation of these two approaches by using the `tl.logsoftmax_sample()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cD8F14b49do1"
      },
      "source": [
        "def next_symbol(model, input_tokens, cur_output_tokens, temperature):\n",
        "    \"\"\"Returns the index of the next token.\n",
        "    Args:\n",
        "        model: the NMT model.\n",
        "        input_tokens (np.ndarray 1 x n_tokens): tokenized representation of the input sentence\n",
        "        cur_output_tokens (list): tokenized representation of previously translated words\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "    Returns:\n",
        "        int: index of the next token in the translated sentence\n",
        "        float: log probability of the next symbol\n",
        "    \"\"\"\n",
        "    # set the length of the current output tokens\n",
        "    token_length = len(cur_output_tokens)\n",
        "    # calculate next power of 2 for padding length \n",
        "    padded_length = np.power(2, int(np.ceil(np.log2(token_length + 1))))\n",
        "    # pad cur_output_tokens up to the padded_length\n",
        "    padded = cur_output_tokens + [0] * (padded_length - token_length) \n",
        "    # model expects the output to have an axis for the batch size in front so\n",
        "    # convert `padded` list to a numpy array with shape (x, <padded_length>) where the\n",
        "    # x position is the batch axis.\n",
        "    padded_with_batch = np.expand_dims(padded, axis=0)\n",
        "    # the model prediction.\n",
        "    output, _ = model((input_tokens, padded_with_batch))   \n",
        "    # get log probabilities from the last token output\n",
        "    log_probs = output[0, token_length, :]\n",
        "    # get the next symbol by getting a logsoftmax sample\n",
        "    symbol = int(tl.logsoftmax_sample(log_probs, temperature))\n",
        "    return symbol, float(log_probs[symbol])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0KlObsa9dpE"
      },
      "source": [
        "The `sampling_decode()` function will call the `next_symbol()` function above several times until the next output is the end-of-sentence token (i.e. `EOS`). It takes in an input string and returns the translated version of that string.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwIB-MQl9dpF"
      },
      "source": [
        "def sampling_decode(input_sentence, model = None, temperature=0.0, vocab_file=None, vocab_dir=None):\n",
        "    \"\"\"Returns the translated sentence.\n",
        "    Args:\n",
        "        input_sentence (str): sentence to translate.\n",
        "        model: the NMT model.\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "        vocab_file (str): filename of the vocabulary\n",
        "        vocab_dir (str): path to the vocabulary file\n",
        "    Returns:\n",
        "        tuple: (list, str, float)\n",
        "            list of int: tokenized version of the translated sentence\n",
        "            float: log probability of the translated sentence\n",
        "            str: the translated sentence\n",
        "    \"\"\"     \n",
        "    # encode the input sentence\n",
        "    input_tokens = tokenize(input_sentence, vocab_file=vocab_file, vocab_dir=vocab_dir)\n",
        "    # initialize the list of output tokens\n",
        "    cur_output_tokens = []\n",
        "    # initialize an integer that represents the current output index\n",
        "    cur_output = 0  \n",
        "    # Set the encoding of the \"end of sentence\" as 1\n",
        "    EOS = 1\n",
        "    # check that the current output is not the end of sentence token\n",
        "    while cur_output != EOS: \n",
        "        # update the current output token by getting the index of the next word\n",
        "        cur_output, log_prob = next_symbol(model, input_tokens, cur_output_tokens, temperature)\n",
        "        # append the current output token to the list of output tokens\n",
        "        cur_output_tokens.append(cur_output) \n",
        "    # detokenize the output tokens\n",
        "    sentence = detokenize(cur_output_tokens, vocab_file=vocab_file, vocab_dir=vocab_dir)\n",
        "    return cur_output_tokens, log_prob, sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diQYEDgF9dpG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40d8f201-3aa6-42fc-ee96-fa09f2c6959f"
      },
      "source": [
        "# Test the function above. Try varying the temperature setting with values from 0 to 1.\n",
        "# Run it several times with each setting and see how often the output changes.\n",
        "sampling_decode(\"Hello.\", model, temperature=0.0, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([9431, 489, 3, 1], 15.834756851196289, 'Bonjour.')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRbgTBWt9dpO"
      },
      "source": [
        "We have set a default value of `0` to the temperature setting in our implementation of `sampling_decode()` above. As you may have noticed in the `logsoftmax_sample()` method, this setting will ultimately result in greedy decoding. This algorithm generates the translation by getting the most probable word at each step. It gets the argmax of the output array of your model and then returns that index. See the testing function and sample inputs below. You'll notice that the output will remain the same each time you run it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1txjY-x9dpP"
      },
      "source": [
        "def greedy_decode_test(sentence, model=None, vocab_file=None, vocab_dir=None):\n",
        "    \"\"\"Prints the input and output of our NMT model using greedy decode\n",
        "    Args:\n",
        "        sentence (str): a custom string.\n",
        "        model: the NMT model.\n",
        "        vocab_file (str): filename of the vocabulary\n",
        "        vocab_dir (str): path to the vocabulary file\n",
        "    Returns:\n",
        "        str: the translated sentence\n",
        "    \"\"\"    \n",
        "    _,_, translated_sentence = sampling_decode(sentence, model, vocab_file=vocab_file, vocab_dir=vocab_dir)   \n",
        "    print(\"English: \", sentence)\n",
        "    print(\"French: \", translated_sentence)\n",
        "    return translated_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7XKz-9I9dpS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89ebe68b-1522-40e7-bc40-ee525c509235"
      },
      "source": [
        "# put a custom string here\n",
        "your_sentence = 'I love languages.'\n",
        "greedy_decode_test(your_sentence, model, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English:  I love languages.\n",
            "French:  J'aime les langues.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8UlR7LS9dpU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40223d3f-77b5-43c5-cdef-42192673211f"
      },
      "source": [
        "greedy_decode_test('You are almost done with the assignment!', model, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English:  You are almost done with the assignment!\n",
            "French:  Vous êtes presque terminé avec le contrat !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf80_9T29dpX"
      },
      "source": [
        "## 4.2  Minimum Bayes-Risk Decoding\n",
        "\n",
        "Getting the most probable token at each step may not necessarily produce the best results. Another approach is to do Minimum Bayes Risk Decoding or MBR. The general steps to implement this are:\n",
        "\n",
        "1. take several random samples\n",
        "2. score each sample against all other samples\n",
        "3. select the one with the highest score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp_qzJ8u9dpX"
      },
      "source": [
        "<a name='4.2.1'></a>\n",
        "### 4.2.1 Generating samples\n",
        "\n",
        "First, let's build a function to generate several samples. You can use the `sampling_decode()` function you developed earlier to do this easily. We want to record the token list and log probability for each sample as these will be needed in the next step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iSRPOrI9dpX"
      },
      "source": [
        "def generate_samples(sentence, n_samples, model=None, temperature=0.6, vocab_file=None, vocab_dir=None):\n",
        "    \"\"\"Generates samples using sampling_decode()\n",
        "    Args:\n",
        "        sentence (str): sentence to translate.\n",
        "        n_samples (int): number of samples to generate\n",
        "        model: the NMT model.\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "        vocab_file (str): filename of the vocabulary\n",
        "        vocab_dir (str): path to the vocabulary file      \n",
        "    Returns:\n",
        "        tuple: (list, list)\n",
        "            list of lists: token list per sample\n",
        "            list of floats: log probability per sample\n",
        "    \"\"\"\n",
        "    # define lists to contain samples and probabilities\n",
        "    samples, log_probs = [], []\n",
        "    # run a for loop to generate n samples\n",
        "    for _ in range(n_samples):\n",
        "        # get a sample using the sampling_decode() function\n",
        "        sample, logp, _ = sampling_decode(sentence, model, temperature, vocab_file=vocab_file, vocab_dir=vocab_dir)\n",
        "        # append the token list to the samples list\n",
        "        samples.append(sample)\n",
        "        # append the log probability to the log_probs list\n",
        "        log_probs.append(logp)               \n",
        "    return samples, log_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlYC8y8H9dpZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "241edf8f-3921-46be-930b-00058cd6efb5"
      },
      "source": [
        "# generate 4 samples with the default temperature (0.6)\n",
        "generate_samples('I love languages.', 4, model, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([[769, 7, 31720, 21, 15267, 3, 1],\n",
              "  [769, 7, 31720, 13, 15267, 3, 1],\n",
              "  [254, 31720, 21, 15267, 3, 1],\n",
              "  [769, 7, 31720, 13, 15267, 3, 1]],\n",
              " [18.705636978149414, 18.2911319732666, 19.461563110351562, 18.2911319732666])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "VR6FLNdcILll",
        "outputId": "b228a403-306f-4c74-b5ea-8c0f129906a4"
      },
      "source": [
        "detokenize([769, 31, 31720, 21, 15267, 3, 1], VOCAB_FILE, VOCAB_DIR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'J’aime les langues.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HonzLcOP9dpb"
      },
      "source": [
        "### 4.2.2 Comparing overlaps\n",
        "\n",
        "Let us now build our functions to compare a sample against another. There are several metrics available and you can try experimenting with any one of these. We will be calculating scores for unigram overlaps. One of the more simple metrics is the [Jaccard similarity](https://en.wikipedia.org/wiki/Jaccard_index) which gets the intersection over union of two sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IB7ipzoZ9dpc"
      },
      "source": [
        "def jaccard_similarity(candidate, reference):\n",
        "    \"\"\"Returns the Jaccard similarity between two token lists\n",
        "    Args:\n",
        "        candidate (list of int): tokenized version of the candidate translation\n",
        "        reference (list of int): tokenized version of the reference translation\n",
        "    Returns:\n",
        "        float: overlap between the two token lists\n",
        "    \"\"\"  \n",
        "    # convert the lists to a set to get the unique tokens\n",
        "    can_unigram_set, ref_unigram_set = set(candidate), set(reference)  \n",
        "    # get the set of tokens common to both candidate and reference\n",
        "    joint_elems = can_unigram_set.intersection(ref_unigram_set)\n",
        "    # get the set of all tokens found in either candidate or reference\n",
        "    all_elems = can_unigram_set.union(ref_unigram_set)\n",
        "    # divide the number of joint elements by the number of all elements\n",
        "    overlap = len(joint_elems) / len(all_elems)\n",
        "    return overlap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZRis5hp9dph"
      },
      "source": [
        "One of the more commonly used metrics in machine translation is the ROUGE score. For unigrams, this is called ROUGE-1 and you can output the scores for both precision and recall when comparing two samples. To get the final score, you will want to compute the F1-score as given by:\n",
        "\n",
        "$$score = 2* \\frac{(precision * recall)}{(precision + recall)}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRhPTgv09dpi"
      },
      "source": [
        "# for making a frequency table easily\n",
        "from collections import Counter\n",
        "\n",
        "def rouge1_similarity(system, reference):\n",
        "    \"\"\"Returns the ROUGE-1 score between two token lists\n",
        "    Args:\n",
        "        system (list of int): tokenized version of the system translation\n",
        "        reference (list of int): tokenized version of the reference translation\n",
        "    Returns:\n",
        "        float: overlap between the two token lists\n",
        "    \"\"\"    \n",
        "    # make a frequency table of the system tokens\n",
        "    sys_counter = Counter(system)   \n",
        "    # make a frequency table of the reference tokens\n",
        "    ref_counter = Counter(reference)\n",
        "    # initialize overlap to 0\n",
        "    overlap = 0\n",
        "    # run a for loop over the sys_counter object\n",
        "    for token in sys_counter:      \n",
        "        # lookup the value of the token in the sys_counter dictionary \n",
        "        token_count_sys = sys_counter.get(token,0)\n",
        "        # lookup the value of the token in the ref_counter dictionary \n",
        "        token_count_ref = ref_counter.get(token,0)\n",
        "        # update the overlap by getting the smaller number between the two token counts above\n",
        "        overlap += min(token_count_sys, token_count_ref) \n",
        "    # get the precision (i.e. number of overlapping tokens / number of system tokens)\n",
        "    precision = overlap / sum(sys_counter.values())    \n",
        "    # get the recall (i.e. number of overlapping tokens / number of reference tokens)\n",
        "    recall = overlap / sum(ref_counter.values()) \n",
        "    if precision + recall != 0:\n",
        "        # compute the f1-score\n",
        "        rouge1_score = 2 * ((precision * recall)/(precision + recall))\n",
        "    else:\n",
        "        rouge1_score = 0 \n",
        "    return rouge1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn3wLqSb9dpp"
      },
      "source": [
        "### 4.2.3 Overall score\n",
        "\n",
        "We will now build a function to generate the overall score for a particular sample. As mentioned earlier, we need to compare each sample with all other samples. For instance, if we generated 30 sentences, we will need to compare sentence 1 to sentences 2 to 30. Then, we compare sentence 2 to sentences 1 and 3 to 30, and so forth. At each step, we get the average score of all comparisons to get the overall score for a particular sample. To illustrate, these will be the steps to generate the scores of a 4-sample list.\n",
        "\n",
        "1. Get similarity score between sample 1 and sample 2\n",
        "2. Get similarity score between sample 1 and sample 3\n",
        "3. Get similarity score between sample 1 and sample 4\n",
        "4. Get average score of the first 3 steps. This will be the overall score of sample 1.\n",
        "5. Iterate and repeat until samples 1 to 4 have overall scores.\n",
        "\n",
        "We will be storing the results in a dictionary for easy lookups."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Umtj0NLX9dpp"
      },
      "source": [
        "def average_overlap(similarity_fn, samples, *ignore_params):\n",
        "    \"\"\"Returns the arithmetic mean of each candidate sentence in the samples\n",
        "    Args:\n",
        "        similarity_fn (function): similarity function used to compute the overlap\n",
        "        samples (list of lists): tokenized version of the translated sentences\n",
        "        *ignore_params: additional parameters will be ignored\n",
        "    Returns:\n",
        "        dict: scores of each sample\n",
        "            key: index of the sample\n",
        "            value: score of the sample\n",
        "    \"\"\"    \n",
        "    # initialize dictionary\n",
        "    scores = {}\n",
        "    # run a for loop for each sample\n",
        "    for index_candidate, candidate in enumerate(samples):    \n",
        "        # initialize overlap to 0.0\n",
        "        overlap = 0.0\n",
        "        # run a for loop for each sample\n",
        "        for index_sample, sample in enumerate(samples): \n",
        "            # skip if the candidate index is the same as the sample index\n",
        "            if index_candidate == index_sample:\n",
        "                continue                \n",
        "            # get the overlap between candidate and sample using the similarity function\n",
        "            sample_overlap = similarity_fn(candidate,sample)            \n",
        "            # add the sample overlap to the total overlap\n",
        "            overlap += sample_overlap            \n",
        "        # get the score for the candidate by computing the average\n",
        "        score = overlap/index_sample        \n",
        "        # save the score in the dictionary. use index as the key.\n",
        "        scores[index_candidate] = score        \n",
        "    return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-w7LL7lm9dpx"
      },
      "source": [
        "It is also common to see the weighted mean being used to calculate the overall score instead of just the arithmetic mean."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o70TS8PG9dpy"
      },
      "source": [
        "def weighted_avg_overlap(similarity_fn, samples, log_probs):\n",
        "    \"\"\"Returns the weighted mean of each candidate sentence in the samples\n",
        "    Args:\n",
        "        samples (list of lists): tokenized version of the translated sentences\n",
        "        log_probs (list of float): log probability of the translated sentences\n",
        "    Returns:\n",
        "        dict: scores of each sample\n",
        "            key: index of the sample\n",
        "            value: score of the sample\n",
        "    \"\"\"\n",
        "    # initialize dictionary\n",
        "    scores = {}   \n",
        "    # run a for loop for each sample\n",
        "    for index_candidate, candidate in enumerate(samples):          \n",
        "        # initialize overlap and weighted sum\n",
        "        overlap, weight_sum = 0.0, 0.0 \n",
        "        # run a for loop for each sample\n",
        "        for index_sample, (sample, logp) in enumerate(zip(samples, log_probs)):\n",
        "            # skip if the candidate index is the same as the sample index            \n",
        "            if index_candidate == index_sample:\n",
        "                continue            \n",
        "            # convert log probability to linear scale\n",
        "            sample_p = float(np.exp(logp))\n",
        "            # update the weighted sum\n",
        "            weight_sum += sample_p\n",
        "            # get the unigram overlap between candidate and sample\n",
        "            sample_overlap = similarity_fn(candidate, sample)           \n",
        "            # update the overlap\n",
        "            overlap += sample_p * sample_overlap        \n",
        "        # get the score for the candidate\n",
        "        score = overlap / weight_sum\n",
        "        # save the score in the dictionary. use index as the key.\n",
        "        scores[index_candidate] = score\n",
        "    return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5jgBrPu9dp4"
      },
      "source": [
        "### 4.2.4 Putting it all together\n",
        "\n",
        "We will now put everything together and develop the `mbr_decode()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S58nPXgY9dp5"
      },
      "source": [
        "def mbr_decode(sentence, n_samples=4, score_fn=weighted_avg_overlap, similarity_fn=rouge1_similarity, model=model,\n",
        "               temperature=0.6, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR):\n",
        "    \"\"\"Returns the translated sentence using Minimum Bayes Risk decoding\n",
        "    Args:\n",
        "        sentence (str): sentence to translate.\n",
        "        n_samples (int): number of samples to generate\n",
        "        score_fn (function): function that generates the score for each sample\n",
        "        similarity_fn (function): function used to compute the overlap between a\n",
        "        pair of samples\n",
        "        model: the NMT model.\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "        vocab_file (str): filename of the vocabulary\n",
        "        vocab_dir (str): path to the vocabulary file\n",
        "    Returns:\n",
        "        str: the translated sentence\n",
        "    \"\"\"\n",
        "    # generate samples\n",
        "    samples, log_probs = generate_samples(sentence, n_samples,\n",
        "                                          model, temperature,\n",
        "                                          vocab_file, vocab_dir)   \n",
        "    # use the scoring function to get a dictionary of scores\n",
        "    scores = score_fn(similarity_fn, samples, log_probs)\n",
        "    # find the key with the highest score\n",
        "    max_index = max(scores, key=scores.get) \n",
        "    # detokenize the token list associated with the max_index\n",
        "    translated_sentence = detokenize(samples[max_index], vocab_file, vocab_dir)\n",
        "    return (translated_sentence, max_index, scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ab1LHo-59dp8"
      },
      "source": [
        "# put a custom string here\n",
        "your_sentence = 'She speaks English, French and German.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhgGWv7c9dp_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae5e00cb-0935-45f7-9fbe-c96f0e12dfc1"
      },
      "source": [
        "mbr_decode(your_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Elle parle anglais, français et Allemand.',\n",
              " 1,\n",
              " {0: 0.909090909090909,\n",
              "  1: 0.9730044973480663,\n",
              "  2: 0.9730044973480663,\n",
              "  3: 0.9730044973480663})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QqyR1Ym6A_Ah",
        "outputId": "eb7db397-28a6-41c7-c44e-0c314574d147"
      },
      "source": [
        "mbr_decode('You have completed the tutorial.')[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Vous avez terminé le tutorial.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPbqDGUY8Vp_"
      },
      "source": [
        "# **Resources**\n",
        "\n",
        "- [Natural Language Processing Specialization](https://www.coursera.org/specializations/natural-language-processing)\n",
        "\n",
        "- [Trax documentation](https://trax-ml.readthedocs.io/en/latest/index.html)\n",
        "\n",
        "- [Trax community](https://gitter.im/trax-ml/community)"
      ]
    }
  ]
}
