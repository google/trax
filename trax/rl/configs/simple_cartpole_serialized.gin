# Copyright 2020 The Trax Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import trax.models
import trax.optimizers
import trax.supervised.trainer_lib
import trax.rl
import trax.rl.space_serializer
import trax.rl.trainers

# Parameters for BoxSpaceSerializer:
# ==============================================================================
BoxSpaceSerializer.precision = 2
BoxSpaceSerializer.max_range = (-10, 10)

# Parameters for MultifactorSchedule:
# ==============================================================================
world_model/MultifactorSchedule.constant = 1.0
world_model/MultifactorSchedule.factors = 'constant * linear_warmup * rsqrt_decay'
world_model/MultifactorSchedule.warmup_steps = 10000

# Parameters for Adafactor:
# ==============================================================================
Adafactor.learning_rate = 1e-2
Adafactor.beta1 = 0.0
Adafactor.decay_rate = 0.8
Adafactor.clipping_threshold = 1.0
Adafactor.epsilon1 = 1e-30
Adafactor.epsilon2 = 0.001
Adafactor.factored = True
Adafactor.multiply_by_parameter_scale = True

# Parameters for TransformerDecoder:
# ==============================================================================
TransformerDecoder.attention_type = @policy/trax.layers.TimeBinCausalAttention
TransformerDecoder.d_model = 64
TransformerDecoder.d_ff = 128
TransformerDecoder.n_layers = 3
TransformerDecoder.n_heads = 1
TransformerDecoder.dropout = 0.0
TransformerDecoder.max_len = 40000

# Parameters for PPO:
# ==============================================================================
PPO.n_optimizer_steps = 32
PPO.optimizer_batch_size = 16
PPO.target_kl = 1000
PPO.boundary = 200
PPO.max_timestep = 200
PPO.max_timestep_eval = 200
PPO.random_seed = None
PPO.gamma = 0.98
PPO.lambda_ = 0.8
PPO.value_weight = 1.0
PPO.entropy_weight = 0.0
PPO.done_frac_for_policy_save = 0
PPO.len_history_for_policy = None
PPO.separate_eval = True
PPO.eval_every_n = 100
PPO.save_every_n = 100
PPO.policy_and_value_model = @trax.models.TransformerDecoder
PPO.policy_and_value_optimizer = @trax.optimizers.Adafactor
PPO.policy_and_value_vocab_size = 128
PPO.trajectory_dump_min_count_per_shard = 8
PPO.print_every_optimizer_steps = 1

## Parameters for TimeBinCausalAttention:
## ==============================================================================
world_model/TimeBinCausalAttention.dropout = 0.1
world_model/TimeBinCausalAttention.bin_length = 16

policy/TimeBinCausalAttention.dropout = 0.0
policy/TimeBinCausalAttention.bin_length = 16

# Parameters for SerializedSequenceSimulatedEnvProblem:
# ==============================================================================
SerializedSequenceSimulatedEnvProblem.model = @world_model/trax.models.TransformerLM
SerializedSequenceSimulatedEnvProblem.reward_fn = @trax.rl.cartpole_reward_fn
SerializedSequenceSimulatedEnvProblem.done_fn = @trax.rl.cartpole_done_fn
SerializedSequenceSimulatedEnvProblem.vocab_size = 128
SerializedSequenceSimulatedEnvProblem.max_trajectory_length = 201
SerializedSequenceSimulatedEnvProblem.significance_decay = 0.8

# Parameters for SimPLe:
# ==============================================================================
SimPLe.policy_trainer_class = @trax.rl.trainers.PPO
SimPLe.n_real_epochs = 10
SimPLe.n_model_initial_train_steps = 10000
SimPLe.n_model_train_steps_per_epoch = 3000
SimPLe.model_train_batch_size = 32
SimPLe.simulated_env_problem_class = @trax.rl.SerializedSequenceSimulatedEnvProblem
SimPLe.simulated_batch_size = 128
SimPLe.n_simulated_epochs = 30
SimPLe.initial_trajectory_mix_prob = 0.9
SimPLe.init_policy_from_world_model = True

# Parameters for TransformerLM:
# ==============================================================================
world_model/TransformerLM.attention_type = @world_model/trax.layers.TimeBinCausalAttention
world_model/TransformerLM.d_model = 64
world_model/TransformerLM.d_ff = 128
world_model/TransformerLM.n_layers = 3
world_model/TransformerLM.n_heads = 1
world_model/TransformerLM.dropout = 0.1
world_model/TransformerLM.max_len = 40000

# Parameters for train:
# ==============================================================================
world_model/train.eval_frequency = 1000
world_model/train.optimizer = @trax.optimizers.Adafactor

# Parameters for train_rl:
# ==============================================================================
train_rl.env_name = "CartPole-v0"
train_rl.n_epochs = 40000
train_rl.trainer_class = @trax.rl.trainers.SimPLe
