{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Hourglass: enwik8 evaluation - final.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "accelerator": "TPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdO8Wj1U5tLN"
   },
   "source": [
    "#### Copyright 2021 Google LLC."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KRaMWbn1BMtI"
   },
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\")\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmkqZ5M69BJS"
   },
   "source": [
    "# Hourglass: enwik8 evaluation [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/trax/blob/master/trax/models/research/examples/hourglass_enwik8.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vlpvzGh-O2N"
   },
   "source": [
    "This notebook was designed to run on TPU.\n",
    "\n",
    "To use TPUs in Colab, click \"Runtime\" on the main menu bar and select Change runtime type. Set \"TPU\" as the hardware accelerator."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tF6OvI7W9Wbn"
   },
   "source": [
    "!gdown https://drive.google.com/uc?id=18wrzKZLBtLuFOHwzuF-7i_p-rD2miE_6\n",
    "!tar -zxvf enwik8_checkpoint.tar.gz"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXBTKGSsegOy"
   },
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yaqpNaQ4ShRP"
   },
   "source": [
    "TRAX_GITHUB_URL = 'git+https://github.com/google/trax.git'\n",
    "!pip install -q --upgrade jax==0.2.21\n",
    "!pip install -q --upgrade jaxlib==0.1.71+cuda111 -f https://storage.googleapis.com/jax-releases/jax_releases.html\n",
    "!pip install -q $TRAX_GITHUB_URL\n",
    "!pip install -q pickle5\n",
    "!pip install -q neptune-client\n",
    "!pip install -q gin"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kVN8e_m9kg3K"
   },
   "source": [
    "# Execute this for a proper TPU setup!\n",
    "# Make sure the Colab Runtime is set to Accelerator: TPU.\n",
    "import jax\n",
    "import requests\n",
    "import os\n",
    "if 'TPU_DRIVER_MODE' not in globals():\n",
    "    url = 'http://' + os.environ['COLAB_TPU_ADDR'].split(':')[0] + ':8475/requestversion/tpu_driver0.1-dev20200416'\n",
    "    resp = requests.post(url)\n",
    "    TPU_DRIVER_MODE = 1\n",
    "\n",
    "# The following is required to use TPU Driver as JAX's backend.\n",
    "from jax.config import config\n",
    "config.FLAGS.jax_xla_backend = \"tpu_driver\"\n",
    "config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']\n",
    "print(config.FLAGS.jax_backend_target)\n",
    "jax.devices()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8GX2u0gekPa"
   },
   "source": [
    "### Download enwik8 dataset and load data\n",
    "\n",
    "A standard script for enwik8 preprocessing is used."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2euURrJBcmKX"
   },
   "source": [
    "!wget --continue http://mattmahoney.net/dc/enwik8.zip\n",
    "!wget https://raw.githubusercontent.com/salesforce/awd-lstm-lm/master/data/enwik8/prep_enwik8.py\n",
    "!python3 prep_enwik8.py"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "P0QGM4-Atvkq"
   },
   "source": [
    "# The checkpoint was trained with python3.8 which uses pickle5, hence this hack.\n",
    "layers_base_path = '/usr/local/lib/python3.7/dist-packages/trax/layers/base.py'\n",
    "with open(layers_base_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "idx = lines.index('import pickle\\n')\n",
    "lines[idx] = 'import pickle5 as pickle\\n'\n",
    "with open(layers_base_path, 'w') as f:\n",
    "    f.writelines(lines)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WKFdaeFuSwvA"
   },
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "from trax.fastmath import numpy as jnp\n",
    "\n",
    "def raw_ds_to_tensor(raw_file_path):\n",
    "    with tf.io.gfile.GFile(raw_file_path, mode='rb') as f:\n",
    "        raw_data = f.read()\n",
    "        print(f'Bytes in {raw_file_path}:', len(raw_data))\n",
    "    return jnp.array(list(raw_data))\n",
    "\n",
    "testset_tensor, validset_tensor = map(raw_ds_to_tensor, [\n",
    "    '/content/test.txt.raw',\n",
    "    '/content/valid.txt.raw',\n",
    "])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Ehl0E2tjhxy"
   },
   "source": [
    "### Load the trained checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QnFfDnQKqstR"
   },
   "source": [
    "import gin\n",
    "import trax\n",
    "\n",
    "MODEL_DIR = 'enwik8_checkpoint'\n",
    "\n",
    "gin.parse_config_file(f'./{MODEL_DIR}/config.gin')\n",
    "\n",
    "model = trax.models.HourglassLM(mode='eval')\n",
    "\n",
    "model.init_from_file(\n",
    "    f'./{MODEL_DIR}/model.pkl.gz',\n",
    "    weights_only=True\n",
    ")\n",
    "\n",
    "loss_fn = trax.layers.WeightedCategoryCrossEntropy()\n",
    "model_eval = trax.layers.Accelerate(trax.layers.Serial(\n",
    "    model,\n",
    "    loss_fn\n",
    "))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OuHGqx9vTVL"
   },
   "source": [
    "### Evaluate on the test set"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uwg6GN3yKvgQ"
   },
   "source": [
    "from trax import fastmath\n",
    "from trax.fastmath import numpy as jnp\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def batched_inputs(data_gen, batch_size):\n",
    "  inp_stack, mask_stack = [], []\n",
    "\n",
    "  for input_example, mask in data_gen:\n",
    "    inp_stack.append(input_example)\n",
    "    mask_stack.append(mask)\n",
    "    if len(inp_stack) % batch_size == 0:\n",
    "      if len(set(len(example) for example in inp_stack)) > 1:\n",
    "        for x, m in zip(inp_stack, mask_stack):\n",
    "          yield x, m\n",
    "      else:\n",
    "        input_batch = jnp.stack(inp_stack)\n",
    "        mask_batch = jnp.stack(mask_stack)\n",
    "\n",
    "        yield input_batch, mask_batch\n",
    "      inp_stack, mask_stack = [], []\n",
    "\n",
    "  if len(inp_stack) > 0:\n",
    "    for inp, mask in zip(inp_stack, mask_stack):\n",
    "      yield inp, mask\n",
    "\n",
    "\n",
    "def run_full_evaluation(accelerated_model_with_loss, examples_data_gen,\n",
    "                        batch_size, pad_to_len=None):\n",
    "  # Important: we assume batch size per device = 1\n",
    "  assert batch_size % fastmath.local_device_count() == 0\n",
    "  assert fastmath.local_device_count() == 1 or \\\n",
    "         batch_size == fastmath.local_device_count()\n",
    "\n",
    "  loss_sum, n_tokens = 0.0, 0\n",
    "\n",
    "  def pad_right(inp_tensor):\n",
    "    if pad_to_len:\n",
    "      return jnp.pad(inp_tensor,\n",
    "                     [[0, 0], [0, max(0, pad_to_len - inp_tensor.shape[1])]])\n",
    "    else:\n",
    "      return inp_tensor\n",
    "\n",
    "  batch_gen = batched_inputs(examples_data_gen, batch_size)\n",
    "\n",
    "  def batch_leftover_example(input_example, example_mask):\n",
    "    def extend_shape_to_batch_size(tensor):\n",
    "      return jnp.repeat(tensor, repeats=batch_size, axis=0)\n",
    "\n",
    "    return map(extend_shape_to_batch_size,\n",
    "               (input_example[None, ...], example_mask[None, ...]))\n",
    "\n",
    "  for i, (inp, mask) in tqdm(enumerate(batch_gen)):\n",
    "    leftover_batch = False\n",
    "    if len(inp.shape) == 1:\n",
    "      inp, mask = batch_leftover_example(inp, mask)\n",
    "      leftover_batch = True\n",
    "\n",
    "    inp, mask = map(pad_right, [inp, mask])\n",
    "\n",
    "    example_losses = accelerated_model_with_loss((inp, inp, mask))\n",
    "\n",
    "    if leftover_batch:\n",
    "      example_losses = example_losses[:1]\n",
    "      mask = mask[:1]\n",
    "\n",
    "    example_lengths = mask.sum(axis=-1)\n",
    "\n",
    "    loss_sum += (example_lengths * example_losses).sum()\n",
    "    n_tokens += mask.sum()\n",
    "\n",
    "    if i % 200 == 0:\n",
    "      print(f'Batches: {i}, current loss: {loss_sum / float(n_tokens)}')\n",
    "\n",
    "  return loss_sum / float(n_tokens)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8B4xaZbaaxN"
   },
   "source": [
    "We evaluate chunks of length $128$ bytes, preceded by a context of $128 \\cdot 31$ bytes (the whole context is $4096$)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F8xAbG6PZfDm",
    "outputId": "050ba05e-2222-4f0b-b6e3-c1d02773d6cf"
   },
   "source": [
    "# Prepare the input generator: it should yield (input, mask) tuples\n",
    "def contextful_eval_data(bytes_tensor, CHUNK_LEN, N_CHUNKS_BEFORE):\n",
    "    for start in range(0, len(bytes_tensor), CHUNK_LEN):\n",
    "        shifted_chunk = bytes_tensor[max(0, start - (N_CHUNKS_BEFORE * CHUNK_LEN)):\n",
    "                                                    start+CHUNK_LEN]\n",
    "        mask = jnp.zeros_like(shifted_chunk)\n",
    "        masked_len = min(CHUNK_LEN, len(bytes_tensor) - start)\n",
    "\n",
    "        mask = fastmath.index_update(mask, jax.ops.index[-masked_len:], 1)\n",
    "\n",
    "        shifted_chunk = trax.data.inputs._pad_to_multiple_of(shifted_chunk,\n",
    "                                                             CHUNK_LEN, axis=0)\n",
    "        mask = trax.data.inputs._pad_to_multiple_of(mask, CHUNK_LEN, axis=0)\n",
    "\n",
    "        yield shifted_chunk, mask\n",
    "\n",
    "# Split the input into chunks of 4096\n",
    "PAD_TO_LEN = 4098 # We need to pad because shorten factor 3 is used.\n",
    "CHUNK_LEN = 128 #\n",
    "N_CHUNKS_BEFORE = 31\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "test_data_gen = contextful_eval_data(testset_tensor, CHUNK_LEN, N_CHUNKS_BEFORE)\n",
    "\n",
    "loss = run_full_evaluation(model_eval, test_data_gen, BATCH_SIZE, PAD_TO_LEN)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "1it [00:01,  1.48s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batches: 0, current loss: 1.1697633266448975\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "201it [03:35,  1.09s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batches: 200, current loss: 0.671483039855957\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "401it [07:11,  1.09s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batches: 400, current loss: 0.6324439644813538\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "601it [10:46,  1.09s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batches: 600, current loss: 0.669732928276062\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "801it [14:22,  1.08s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batches: 800, current loss: 0.690700888633728\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "1001it [17:59,  1.08s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batches: 1000, current loss: 0.7042409777641296\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "1201it [21:35,  1.07s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batches: 1200, current loss: 0.7051951885223389\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "1401it [25:09,  1.06s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batches: 1400, current loss: 0.7044456005096436\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "1601it [28:45,  1.12s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batches: 1600, current loss: 0.7035351991653442\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "1801it [32:20,  1.07s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batches: 1800, current loss: 0.690122663974762\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2001it [35:55,  1.07s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batches: 2000, current loss: 0.6649767756462097\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2201it [39:30,  1.07s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batches: 2200, current loss: 0.6716358661651611\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2401it [43:04,  1.08s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batches: 2400, current loss: 0.6756933331489563\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2601it [46:39,  1.08s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batches: 2600, current loss: 0.6825714707374573\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2801it [50:13,  1.06s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batches: 2800, current loss: 0.6843773722648621\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "3001it [53:46,  1.06s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batches: 3000, current loss: 0.6865374445915222\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "3201it [57:22,  1.09s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batches: 3200, current loss: 0.6855794787406921\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "3401it [1:00:58,  1.07s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batches: 3400, current loss: 0.6887989640235901\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "3601it [1:04:32,  1.06s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batches: 3600, current loss: 0.688316822052002\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "3801it [1:08:05,  1.06s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batches: 3800, current loss: 0.6921071410179138\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "4001it [1:11:40,  1.07s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batches: 4000, current loss: 0.6904897093772888\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "4201it [1:15:16,  1.07s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batches: 4200, current loss: 0.6908246278762817\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "4401it [1:18:52,  1.07s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batches: 4400, current loss: 0.6909059882164001\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "4601it [1:22:26,  1.20s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batches: 4600, current loss: 0.6896733045578003\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "4801it [1:26:02,  1.08s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batches: 4800, current loss: 0.6903342604637146\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "4917it [1:28:07,  1.08s/it]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x_GynuimYNxl",
    "outputId": "6dd21426-7852-4f37-b4eb-e0ae67633ddb"
   },
   "source": [
    "print(f'Final perplexity: {loss}, final bpd: {loss / jnp.log(2)}')"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Final perplexity: 0.6918511986732483, final bpd: 0.9981303811073303\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DvYUpo4Xji5"
   },
   "source": [
    "### Generate text from the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0YMco8prXiXn"
   },
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def autoregressive_sample(model, temp=1.0, batch_size=8, l=3072, vocab_size=256):\n",
    "  model = trax.layers.Accelerate(model)\n",
    "  x = np.zeros((batch_size, l), dtype=np.int32)\n",
    "\n",
    "  logits_prev = np.zeros((batch_size, l, vocab_size), dtype=np.float32)\n",
    "  for i in tqdm(range(l)):\n",
    "    logits = model(x)\n",
    "    np.testing.assert_array_almost_equal(logits_prev[:, :i], logits[:, :i])\n",
    "    logits_prev = logits\n",
    "\n",
    "    sample = trax.layers.logsoftmax_sample(logits[:, i, :], temperature=temp)\n",
    "    x[:, i] = sample\n",
    "  return x"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dPg_NysgamHO",
    "outputId": "b788ad88-f121-4d2d-d2a6-15f99f63cd6b"
   },
   "source": [
    "samples = autoregressive_sample(model, l=1026)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 513/513 [05:47<00:00,  1.48it/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "id": "Hwtw9vXNc7h6",
    "outputId": "759165a1-ee58-4f51-887e-f27ee551a51e"
   },
   "source": [
    "bytes((samples[0]).tolist()).decode()"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' the political laws.  War also helped develop the [[Soviet Union|Soviet]] system in western Europe, as did [[Luxembourg]] and the Church of [[Sweden]]. The immediate impact of nuclear war took place in eastern Europe, bankrupted by serious [[Nuclear fallout|fallout]] from [[Early Modern Europe|miners from both sides]]. The state ally strategic eastern Europe had immediately concluded the war in both sides, although it was extremely weak. Meanwhile, the US was developing an urban and commercial life of more t'"
      ]
     },
     "metadata": {},
     "execution_count": 158
    }
   ]
  }
 ]
}